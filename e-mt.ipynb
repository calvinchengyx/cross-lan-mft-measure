{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### import necessary libraries #######\n",
    "# %pip install spacy==3.4.0\n",
    "# %pip install pandas==2.0.0\n",
    "# %pip install https://github.com/medianeuroscience/emfdscore/archive/master.zip\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# import nltk\n",
    "# nltk.download('punkt') # Download the punkt tokenizer models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 1. Machine Translation\n",
    "- we use `google_trans_en` to measure and then compare with `source_label` column\n",
    "- all enlglish translations are done by Google Translate API, i skipped this step here as it is fairly simple and straightforward. it was done on spreadsheet before importing to python\n",
    "- emfd is applied directly via [github repo](https://github.com/medianeuroscience/emfdscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['text', 'source', 'source_lan', 'source_label', 'google_trans_en',\n",
      "       'source_dataset'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# 0. load all benchmarking dataset and check the structure \n",
    "df = pd.read_csv('path to BM.csv', dtype=str)\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## E 1.1 Word Frequency - MFD, MFD2, and eMFD dictionary \n",
    "* tutorial website; [emfd](https://github.com/medianeuroscience/emfdscore/blob/master/eMFDscore_Tutorial.ipynb)\n",
    "* i used method1 - Use All Probabilities per Word and Return Sentiment Scores\n",
    "* the github repo has a tutorial on how to use all three dictionaries, perfect\n",
    "* be careful with the version of `pandas` and `spacy`, check the issue page to make sure you are using the right version that is compatible with the emfdscore package. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3088, 1)\n"
     ]
    }
   ],
   "source": [
    "df = df[['google_trans_en']]\n",
    "df.to_csv('/workspaces/llms_mft_multilingual/experiment1/benchmark_dict.csv', index=False)\n",
    "template_input = pd.read_csv('/workspaces/llms_mft_multilingual/experiment1/benchmark_dict.csv', header=None)\n",
    "print(template_input.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed: 0   0% |                      | Elapsed Time: 0:00:00 ETA:  --:--:--\n",
      "Processed: 37   1% |                     | Elapsed Time: 0:00:00 ETA:   0:00:08\n",
      "Processed: 74   2% |                     | Elapsed Time: 0:00:00 ETA:   0:00:08\n",
      "Processed: 111   3% |                    | Elapsed Time: 0:00:00 ETA:   0:00:08\n",
      "Processed: 150   4% |                    | Elapsed Time: 0:00:00 ETA:   0:00:07\n",
      "Processed: 189   6% |❤                   | Elapsed Time: 0:00:00 ETA:   0:00:07\n",
      "Processed: 218   7% |❤                   | Elapsed Time: 0:00:00 ETA:   0:00:08\n",
      "Processed: 255   8% |❤                   | Elapsed Time: 0:00:00 ETA:   0:00:07\n",
      "Processed: 274   8% |❤                   | Elapsed Time: 0:00:00 ETA:   0:00:07\n",
      "Processed: 311  10% |❤❤                  | Elapsed Time: 0:00:00 ETA:   0:00:07\n",
      "Processed: 350  11% |❤❤                  | Elapsed Time: 0:00:00 ETA:   0:00:07\n",
      "Processed: 385  12% |❤❤                  | Elapsed Time: 0:00:01 ETA:   0:00:07\n",
      "Processed: 408  13% |❤❤                  | Elapsed Time: 0:00:01 ETA:   0:00:07\n",
      "Processed: 430  13% |❤❤                  | Elapsed Time: 0:00:01 ETA:   0:00:07\n",
      "Processed: 467  15% |❤❤❤                 | Elapsed Time: 0:00:01 ETA:   0:00:07\n",
      "Processed: 485  15% |❤❤❤                 | Elapsed Time: 0:00:01 ETA:   0:00:07\n",
      "Processed: 509  16% |❤❤❤                 | Elapsed Time: 0:00:01 ETA:   0:00:07\n",
      "Processed: 545  17% |❤❤❤                 | Elapsed Time: 0:00:01 ETA:   0:00:07\n",
      "Processed: 587  19% |❤❤❤                 | Elapsed Time: 0:00:01 ETA:   0:00:07\n",
      "Processed: 626  20% |❤❤❤❤                | Elapsed Time: 0:00:01 ETA:   0:00:07\n",
      "Processed: 664  21% |❤❤❤❤                | Elapsed Time: 0:00:01 ETA:   0:00:06\n",
      "Processed: 704  22% |❤❤❤❤                | Elapsed Time: 0:00:01 ETA:   0:00:06\n",
      "Processed: 743  24% |❤❤❤❤                | Elapsed Time: 0:00:02 ETA:   0:00:06\n",
      "Processed: 782  25% |❤❤❤❤❤               | Elapsed Time: 0:00:02 ETA:   0:00:06\n",
      "Processed: 810  26% |❤❤❤❤❤               | Elapsed Time: 0:00:02 ETA:   0:00:06\n",
      "Processed: 836  27% |❤❤❤❤❤               | Elapsed Time: 0:00:02 ETA:   0:00:06\n",
      "Processed: 860  27% |❤❤❤❤❤               | Elapsed Time: 0:00:02 ETA:   0:00:06\n",
      "Processed: 900  29% |❤❤❤❤❤               | Elapsed Time: 0:00:02 ETA:   0:00:06\n",
      "Processed: 939  30% |❤❤❤❤❤❤              | Elapsed Time: 0:00:02 ETA:   0:00:06\n",
      "Processed: 978  31% |❤❤❤❤❤❤              | Elapsed Time: 0:00:02 ETA:   0:00:05\n",
      "Processed: 1017  32% |❤❤❤❤❤❤             | Elapsed Time: 0:00:02 ETA:   0:00:05\n",
      "Processed: 1051  34% |❤❤❤❤❤❤             | Elapsed Time: 0:00:02 ETA:   0:00:05\n",
      "Processed: 1076  34% |❤❤❤❤❤❤             | Elapsed Time: 0:00:03 ETA:   0:00:05\n",
      "Processed: 1095  35% |❤❤❤❤❤❤             | Elapsed Time: 0:00:03 ETA:   0:00:05\n",
      "Processed: 1134  36% |❤❤❤❤❤❤             | Elapsed Time: 0:00:03 ETA:   0:00:05\n",
      "Processed: 1173  37% |❤❤❤❤❤❤❤            | Elapsed Time: 0:00:03 ETA:   0:00:05\n",
      "Processed: 1212  39% |❤❤❤❤❤❤❤            | Elapsed Time: 0:00:03 ETA:   0:00:05\n",
      "Processed: 1251  40% |❤❤❤❤❤❤❤            | Elapsed Time: 0:00:03 ETA:   0:00:05\n",
      "Processed: 1290  41% |❤❤❤❤❤❤❤            | Elapsed Time: 0:00:03 ETA:   0:00:04\n",
      "Processed: 1316  42% |❤❤❤❤❤❤❤❤           | Elapsed Time: 0:00:03 ETA:   0:00:04\n",
      "Processed: 1353  43% |❤❤❤❤❤❤❤❤           | Elapsed Time: 0:00:03 ETA:   0:00:04\n",
      "Processed: 1394  45% |❤❤❤❤❤❤❤❤           | Elapsed Time: 0:00:03 ETA:   0:00:04\n",
      "Processed: 1437  46% |❤❤❤❤❤❤❤❤           | Elapsed Time: 0:00:03 ETA:   0:00:04\n",
      "Processed: 1481  47% |❤❤❤❤❤❤❤❤❤          | Elapsed Time: 0:00:04 ETA:   0:00:04\n",
      "Processed: 1523  49% |❤❤❤❤❤❤❤❤❤          | Elapsed Time: 0:00:04 ETA:   0:00:04\n",
      "Processed: 1544  50% |❤❤❤❤❤❤❤❤❤          | Elapsed Time: 0:00:04 ETA:   0:00:04\n",
      "Processed: 1564  50% |❤❤❤❤❤❤❤❤❤          | Elapsed Time: 0:00:04 ETA:   0:00:04\n",
      "Processed: 1595  51% |❤❤❤❤❤❤❤❤❤          | Elapsed Time: 0:00:04 ETA:   0:00:04\n",
      "Processed: 1624  52% |❤❤❤❤❤❤❤❤❤          | Elapsed Time: 0:00:04 ETA:   0:00:04\n",
      "Processed: 1642  53% |❤❤❤❤❤❤❤❤❤❤         | Elapsed Time: 0:00:04 ETA:   0:00:04\n",
      "Processed: 1677  54% |❤❤❤❤❤❤❤❤❤❤         | Elapsed Time: 0:00:04 ETA:   0:00:03\n",
      "Processed: 1712  55% |❤❤❤❤❤❤❤❤❤❤         | Elapsed Time: 0:00:04 ETA:   0:00:03\n",
      "Processed: 1744  56% |❤❤❤❤❤❤❤❤❤❤         | Elapsed Time: 0:00:04 ETA:   0:00:03\n",
      "Processed: 1775  57% |❤❤❤❤❤❤❤❤❤❤         | Elapsed Time: 0:00:04 ETA:   0:00:03\n",
      "Processed: 1799  58% |❤❤❤❤❤❤❤❤❤❤❤        | Elapsed Time: 0:00:05 ETA:   0:00:03\n",
      "Processed: 1827  59% |❤❤❤❤❤❤❤❤❤❤❤        | Elapsed Time: 0:00:05 ETA:   0:00:03\n",
      "Processed: 1855  60% |❤❤❤❤❤❤❤❤❤❤❤        | Elapsed Time: 0:00:05 ETA:   0:00:03\n",
      "Processed: 1877  60% |❤❤❤❤❤❤❤❤❤❤❤        | Elapsed Time: 0:00:05 ETA:   0:00:03\n",
      "Processed: 1907  61% |❤❤❤❤❤❤❤❤❤❤❤        | Elapsed Time: 0:00:05 ETA:   0:00:03\n",
      "Processed: 1936  62% |❤❤❤❤❤❤❤❤❤❤❤        | Elapsed Time: 0:00:05 ETA:   0:00:03\n",
      "Processed: 1955  63% |❤❤❤❤❤❤❤❤❤❤❤❤       | Elapsed Time: 0:00:05 ETA:   0:00:03\n",
      "Processed: 1979  64% |❤❤❤❤❤❤❤❤❤❤❤❤       | Elapsed Time: 0:00:05 ETA:   0:00:03\n",
      "Processed: 2013  65% |❤❤❤❤❤❤❤❤❤❤❤❤       | Elapsed Time: 0:00:05 ETA:   0:00:03\n",
      "Processed: 2033  65% |❤❤❤❤❤❤❤❤❤❤❤❤       | Elapsed Time: 0:00:05 ETA:   0:00:03\n",
      "Processed: 2069  67% |❤❤❤❤❤❤❤❤❤❤❤❤       | Elapsed Time: 0:00:05 ETA:   0:00:02\n",
      "Processed: 2102  68% |❤❤❤❤❤❤❤❤❤❤❤❤       | Elapsed Time: 0:00:06 ETA:   0:00:02\n",
      "Processed: 2130  68% |❤❤❤❤❤❤❤❤❤❤❤❤❤      | Elapsed Time: 0:00:06 ETA:   0:00:02\n",
      "Processed: 2150  69% |❤❤❤❤❤❤❤❤❤❤❤❤❤      | Elapsed Time: 0:00:06 ETA:   0:00:02\n",
      "Processed: 2185  70% |❤❤❤❤❤❤❤❤❤❤❤❤❤      | Elapsed Time: 0:00:06 ETA:   0:00:02\n",
      "Processed: 2217  71% |❤❤❤❤❤❤❤❤❤❤❤❤❤      | Elapsed Time: 0:00:06 ETA:   0:00:02\n",
      "Processed: 2250  72% |❤❤❤❤❤❤❤❤❤❤❤❤❤      | Elapsed Time: 0:00:06 ETA:   0:00:02\n",
      "Processed: 2268  73% |❤❤❤❤❤❤❤❤❤❤❤❤❤      | Elapsed Time: 0:00:06 ETA:   0:00:02\n",
      "Processed: 2300  74% |❤❤❤❤❤❤❤❤❤❤❤❤❤❤     | Elapsed Time: 0:00:06 ETA:   0:00:02\n",
      "Processed: 2329  75% |❤❤❤❤❤❤❤❤❤❤❤❤❤❤     | Elapsed Time: 0:00:06 ETA:   0:00:02\n",
      "Processed: 2346  75% |❤❤❤❤❤❤❤❤❤❤❤❤❤❤     | Elapsed Time: 0:00:06 ETA:   0:00:02\n",
      "Processed: 2378  77% |❤❤❤❤❤❤❤❤❤❤❤❤❤❤     | Elapsed Time: 0:00:06 ETA:   0:00:02\n",
      "Processed: 2409  78% |❤❤❤❤❤❤❤❤❤❤❤❤❤❤     | Elapsed Time: 0:00:07 ETA:   0:00:01\n",
      "Processed: 2444  79% |❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤    | Elapsed Time: 0:00:07 ETA:   0:00:01\n",
      "Processed: 2463  79% |❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤    | Elapsed Time: 0:00:07 ETA:   0:00:01\n",
      "Processed: 2496  80% |❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤    | Elapsed Time: 0:00:07 ETA:   0:00:01\n",
      "Processed: 2519  81% |❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤    | Elapsed Time: 0:00:07 ETA:   0:00:01\n",
      "Processed: 2541  82% |❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤    | Elapsed Time: 0:00:07 ETA:   0:00:01\n",
      "Processed: 2571  83% |❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤    | Elapsed Time: 0:00:07 ETA:   0:00:01\n",
      "Processed: 2606  84% |❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤   | Elapsed Time: 0:00:07 ETA:   0:00:01\n",
      "Processed: 2641  85% |❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤   | Elapsed Time: 0:00:07 ETA:   0:00:01\n",
      "Processed: 2679  86% |❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤   | Elapsed Time: 0:00:07 ETA:   0:00:01\n",
      "Processed: 2698  87% |❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤   | Elapsed Time: 0:00:08 ETA:   0:00:01\n",
      "Processed: 2727  88% |❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤   | Elapsed Time: 0:00:08 ETA:   0:00:01\n",
      "Processed: 2755  89% |❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤   | Elapsed Time: 0:00:08 ETA:   0:00:00\n",
      "Processed: 2776  89% |❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤  | Elapsed Time: 0:00:08 ETA:   0:00:00\n",
      "Processed: 2801  90% |❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤  | Elapsed Time: 0:00:08 ETA:   0:00:00\n",
      "Processed: 2828  91% |❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤  | Elapsed Time: 0:00:08 ETA:   0:00:00\n",
      "Processed: 2854  92% |❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤  | Elapsed Time: 0:00:08 ETA:   0:00:00\n",
      "Processed: 2881  93% |❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤  | Elapsed Time: 0:00:08 ETA:   0:00:00\n",
      "Processed: 2907  94% |❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤  | Elapsed Time: 0:00:08 ETA:   0:00:00\n",
      "Processed: 2930  94% |❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤ | Elapsed Time: 0:00:08 ETA:   0:00:00\n",
      "Processed: 2949  95% |❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤ | Elapsed Time: 0:00:09 ETA:   0:00:00\n",
      "Processed: 2968  96% |❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤ | Elapsed Time: 0:00:09 ETA:   0:00:00\n",
      "Processed: 2997  97% |❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤ | Elapsed Time: 0:00:09 ETA:   0:00:00\n",
      "Processed: 3010  97% |❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤ | Elapsed Time: 0:00:09 ETA:   0:00:00\n",
      "Processed: 3037  98% |❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤ | Elapsed Time: 0:00:09 ETA:   0:00:00\n",
      "Processed: 3066  99% |❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤ | Elapsed Time: 0:00:09 ETA:   0:00:00\n",
      "Processed: 3088 100% |❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤| Elapsed Time: 0:00:09 Time:  0:00:09\n"
     ]
    }
   ],
   "source": [
    "from emfdscore.scoring import score_docs \n",
    "\n",
    "# template_input.head()\n",
    "num_docs = len(template_input)\n",
    "\n",
    "DICT_TYPE = 'emfd'\n",
    "PROB_MAP = 'all'\n",
    "SCORE_METHOD = 'bow'\n",
    "OUT_METRICS = 'sentiment'\n",
    "OUT_CSV_PATH = 'emfd_score.csv'\n",
    "\n",
    "emfd_score = score_docs(template_input, DICT_TYPE, PROB_MAP, SCORE_METHOD, OUT_METRICS, num_docs)\n",
    "emfd_score.to_csv(OUT_CSV_PATH, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed: 0   0% |                      | Elapsed Time: 0:00:00 ETA:  --:--:--\n",
      "Processed: 36   1% |                     | Elapsed Time: 0:00:00 ETA:   0:00:08\n",
      "Processed: 71   2% |                     | Elapsed Time: 0:00:00 ETA:   0:00:08\n",
      "Processed: 102   3% |                    | Elapsed Time: 0:00:00 ETA:   0:00:08\n",
      "Processed: 141   4% |                    | Elapsed Time: 0:00:00 ETA:   0:00:08\n",
      "Processed: 180   5% |❤                   | Elapsed Time: 0:00:00 ETA:   0:00:08\n",
      "Processed: 196   6% |❤                   | Elapsed Time: 0:00:00 ETA:   0:00:08\n",
      "Processed: 234   7% |❤                   | Elapsed Time: 0:00:00 ETA:   0:00:08\n",
      "Processed: 273   8% |❤                   | Elapsed Time: 0:00:00 ETA:   0:00:08\n",
      "Processed: 312  10% |❤❤                  | Elapsed Time: 0:00:00 ETA:   0:00:07\n",
      "Processed: 345  11% |❤❤                  | Elapsed Time: 0:00:00 ETA:   0:00:07\n",
      "Processed: 379  12% |❤❤                  | Elapsed Time: 0:00:01 ETA:   0:00:07\n",
      "Processed: 414  13% |❤❤                  | Elapsed Time: 0:00:01 ETA:   0:00:07\n",
      "Processed: 453  14% |❤❤                  | Elapsed Time: 0:00:01 ETA:   0:00:07\n",
      "Processed: 475  15% |❤❤❤                 | Elapsed Time: 0:00:01 ETA:   0:00:07\n",
      "Processed: 491  15% |❤❤❤                 | Elapsed Time: 0:00:01 ETA:   0:00:07\n",
      "Processed: 536  17% |❤❤❤                 | Elapsed Time: 0:00:01 ETA:   0:00:07\n",
      "Processed: 580  18% |❤❤❤                 | Elapsed Time: 0:00:01 ETA:   0:00:07\n",
      "Processed: 623  20% |❤❤❤❤                | Elapsed Time: 0:00:01 ETA:   0:00:07\n",
      "Processed: 663  21% |❤❤❤❤                | Elapsed Time: 0:00:01 ETA:   0:00:06\n",
      "Processed: 704  22% |❤❤❤❤                | Elapsed Time: 0:00:01 ETA:   0:00:06\n",
      "Processed: 743  24% |❤❤❤❤                | Elapsed Time: 0:00:02 ETA:   0:00:06\n",
      "Processed: 782  25% |❤❤❤❤❤               | Elapsed Time: 0:00:02 ETA:   0:00:06\n",
      "Processed: 808  26% |❤❤❤❤❤               | Elapsed Time: 0:00:02 ETA:   0:00:06\n",
      "Processed: 821  26% |❤❤❤❤❤               | Elapsed Time: 0:00:02 ETA:   0:00:06\n",
      "Processed: 860  27% |❤❤❤❤❤               | Elapsed Time: 0:00:02 ETA:   0:00:06\n",
      "Processed: 900  29% |❤❤❤❤❤               | Elapsed Time: 0:00:02 ETA:   0:00:06\n",
      "Processed: 939  30% |❤❤❤❤❤❤              | Elapsed Time: 0:00:02 ETA:   0:00:06\n",
      "Processed: 978  31% |❤❤❤❤❤❤              | Elapsed Time: 0:00:02 ETA:   0:00:05\n",
      "Processed: 1017  32% |❤❤❤❤❤❤             | Elapsed Time: 0:00:02 ETA:   0:00:05\n",
      "Processed: 1051  34% |❤❤❤❤❤❤             | Elapsed Time: 0:00:02 ETA:   0:00:05\n",
      "Processed: 1079  34% |❤❤❤❤❤❤             | Elapsed Time: 0:00:03 ETA:   0:00:05\n",
      "Processed: 1125  36% |❤❤❤❤❤❤             | Elapsed Time: 0:00:03 ETA:   0:00:05\n",
      "Processed: 1169  37% |❤❤❤❤❤❤❤            | Elapsed Time: 0:00:03 ETA:   0:00:05\n",
      "Processed: 1209  39% |❤❤❤❤❤❤❤            | Elapsed Time: 0:00:03 ETA:   0:00:05\n",
      "Processed: 1251  40% |❤❤❤❤❤❤❤            | Elapsed Time: 0:00:03 ETA:   0:00:05\n",
      "Processed: 1290  41% |❤❤❤❤❤❤❤            | Elapsed Time: 0:00:03 ETA:   0:00:04\n",
      "Processed: 1316  42% |❤❤❤❤❤❤❤❤           | Elapsed Time: 0:00:03 ETA:   0:00:04\n",
      "Processed: 1330  43% |❤❤❤❤❤❤❤❤           | Elapsed Time: 0:00:03 ETA:   0:00:04\n",
      "Processed: 1369  44% |❤❤❤❤❤❤❤❤           | Elapsed Time: 0:00:03 ETA:   0:00:04\n",
      "Processed: 1408  45% |❤❤❤❤❤❤❤❤           | Elapsed Time: 0:00:03 ETA:   0:00:04\n",
      "Processed: 1447  46% |❤❤❤❤❤❤❤❤           | Elapsed Time: 0:00:03 ETA:   0:00:04\n",
      "Processed: 1486  48% |❤❤❤❤❤❤❤❤❤          | Elapsed Time: 0:00:04 ETA:   0:00:04\n",
      "Processed: 1524  49% |❤❤❤❤❤❤❤❤❤          | Elapsed Time: 0:00:04 ETA:   0:00:04\n",
      "Processed: 1543  49% |❤❤❤❤❤❤❤❤❤          | Elapsed Time: 0:00:04 ETA:   0:00:04\n",
      "Processed: 1564  50% |❤❤❤❤❤❤❤❤❤          | Elapsed Time: 0:00:04 ETA:   0:00:04\n",
      "Processed: 1587  51% |❤❤❤❤❤❤❤❤❤          | Elapsed Time: 0:00:04 ETA:   0:00:04\n",
      "Processed: 1618  52% |❤❤❤❤❤❤❤❤❤          | Elapsed Time: 0:00:04 ETA:   0:00:04\n",
      "Processed: 1642  53% |❤❤❤❤❤❤❤❤❤❤         | Elapsed Time: 0:00:04 ETA:   0:00:04\n",
      "Processed: 1676  54% |❤❤❤❤❤❤❤❤❤❤         | Elapsed Time: 0:00:04 ETA:   0:00:03\n",
      "Processed: 1700  55% |❤❤❤❤❤❤❤❤❤❤         | Elapsed Time: 0:00:04 ETA:   0:00:03\n",
      "Processed: 1720  55% |❤❤❤❤❤❤❤❤❤❤         | Elapsed Time: 0:00:04 ETA:   0:00:03\n",
      "Processed: 1751  56% |❤❤❤❤❤❤❤❤❤❤         | Elapsed Time: 0:00:04 ETA:   0:00:03\n",
      "Processed: 1784  57% |❤❤❤❤❤❤❤❤❤❤         | Elapsed Time: 0:00:05 ETA:   0:00:03\n",
      "Processed: 1818  58% |❤❤❤❤❤❤❤❤❤❤❤        | Elapsed Time: 0:00:05 ETA:   0:00:03\n",
      "Processed: 1838  59% |❤❤❤❤❤❤❤❤❤❤❤        | Elapsed Time: 0:00:05 ETA:   0:00:03\n",
      "Processed: 1868  60% |❤❤❤❤❤❤❤❤❤❤❤        | Elapsed Time: 0:00:05 ETA:   0:00:03\n",
      "Processed: 1901  61% |❤❤❤❤❤❤❤❤❤❤❤        | Elapsed Time: 0:00:05 ETA:   0:00:03\n",
      "Processed: 1916  62% |❤❤❤❤❤❤❤❤❤❤❤        | Elapsed Time: 0:00:05 ETA:   0:00:03\n",
      "Processed: 1945  62% |❤❤❤❤❤❤❤❤❤❤❤        | Elapsed Time: 0:00:05 ETA:   0:00:03\n",
      "Processed: 1976  63% |❤❤❤❤❤❤❤❤❤❤❤❤       | Elapsed Time: 0:00:05 ETA:   0:00:03\n",
      "Processed: 1994  64% |❤❤❤❤❤❤❤❤❤❤❤❤       | Elapsed Time: 0:00:05 ETA:   0:00:03\n",
      "Processed: 2026  65% |❤❤❤❤❤❤❤❤❤❤❤❤       | Elapsed Time: 0:00:05 ETA:   0:00:03\n",
      "Processed: 2061  66% |❤❤❤❤❤❤❤❤❤❤❤❤       | Elapsed Time: 0:00:05 ETA:   0:00:02\n",
      "Processed: 2092  67% |❤❤❤❤❤❤❤❤❤❤❤❤       | Elapsed Time: 0:00:06 ETA:   0:00:02\n",
      "Processed: 2111  68% |❤❤❤❤❤❤❤❤❤❤❤❤       | Elapsed Time: 0:00:06 ETA:   0:00:02\n",
      "Processed: 2138  69% |❤❤❤❤❤❤❤❤❤❤❤❤❤      | Elapsed Time: 0:00:06 ETA:   0:00:02\n",
      "Processed: 2171  70% |❤❤❤❤❤❤❤❤❤❤❤❤❤      | Elapsed Time: 0:00:06 ETA:   0:00:02\n",
      "Processed: 2189  70% |❤❤❤❤❤❤❤❤❤❤❤❤❤      | Elapsed Time: 0:00:06 ETA:   0:00:02\n",
      "Processed: 2210  71% |❤❤❤❤❤❤❤❤❤❤❤❤❤      | Elapsed Time: 0:00:06 ETA:   0:00:02\n",
      "Processed: 2229  72% |❤❤❤❤❤❤❤❤❤❤❤❤❤      | Elapsed Time: 0:00:06 ETA:   0:00:02\n",
      "Processed: 2254  72% |❤❤❤❤❤❤❤❤❤❤❤❤❤      | Elapsed Time: 0:00:06 ETA:   0:00:02\n",
      "Processed: 2268  73% |❤❤❤❤❤❤❤❤❤❤❤❤❤      | Elapsed Time: 0:00:06 ETA:   0:00:02\n",
      "Processed: 2298  74% |❤❤❤❤❤❤❤❤❤❤❤❤❤❤     | Elapsed Time: 0:00:06 ETA:   0:00:02\n",
      "Processed: 2321  75% |❤❤❤❤❤❤❤❤❤❤❤❤❤❤     | Elapsed Time: 0:00:06 ETA:   0:00:02\n",
      "Processed: 2346  75% |❤❤❤❤❤❤❤❤❤❤❤❤❤❤     | Elapsed Time: 0:00:07 ETA:   0:00:02\n",
      "Processed: 2378  77% |❤❤❤❤❤❤❤❤❤❤❤❤❤❤     | Elapsed Time: 0:00:07 ETA:   0:00:02\n",
      "Processed: 2416  78% |❤❤❤❤❤❤❤❤❤❤❤❤❤❤     | Elapsed Time: 0:00:07 ETA:   0:00:02\n",
      "Processed: 2451  79% |❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤    | Elapsed Time: 0:00:07 ETA:   0:00:01\n",
      "Processed: 2483  80% |❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤    | Elapsed Time: 0:00:07 ETA:   0:00:01\n",
      "Processed: 2502  81% |❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤    | Elapsed Time: 0:00:07 ETA:   0:00:01\n",
      "Processed: 2537  82% |❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤    | Elapsed Time: 0:00:07 ETA:   0:00:01\n",
      "Processed: 2569  83% |❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤    | Elapsed Time: 0:00:07 ETA:   0:00:01\n",
      "Processed: 2604  84% |❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤   | Elapsed Time: 0:00:07 ETA:   0:00:01\n",
      "Processed: 2619  84% |❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤   | Elapsed Time: 0:00:07 ETA:   0:00:01\n",
      "Processed: 2657  86% |❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤   | Elapsed Time: 0:00:07 ETA:   0:00:01\n",
      "Processed: 2681  86% |❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤   | Elapsed Time: 0:00:08 ETA:   0:00:01\n",
      "Processed: 2698  87% |❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤   | Elapsed Time: 0:00:08 ETA:   0:00:01\n",
      "Processed: 2724  88% |❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤   | Elapsed Time: 0:00:08 ETA:   0:00:01\n",
      "Processed: 2753  89% |❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤   | Elapsed Time: 0:00:08 ETA:   0:00:01\n",
      "Processed: 2776  89% |❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤  | Elapsed Time: 0:00:08 ETA:   0:00:00\n",
      "Processed: 2797  90% |❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤  | Elapsed Time: 0:00:08 ETA:   0:00:00\n",
      "Processed: 2815  91% |❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤  | Elapsed Time: 0:00:08 ETA:   0:00:00\n",
      "Processed: 2842  92% |❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤  | Elapsed Time: 0:00:08 ETA:   0:00:00\n",
      "Processed: 2868  92% |❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤  | Elapsed Time: 0:00:08 ETA:   0:00:00\n",
      "Processed: 2893  93% |❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤  | Elapsed Time: 0:00:08 ETA:   0:00:00\n",
      "Processed: 2922  94% |❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤  | Elapsed Time: 0:00:09 ETA:   0:00:00\n",
      "Processed: 2946  95% |❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤ | Elapsed Time: 0:00:09 ETA:   0:00:00\n",
      "Processed: 2971  96% |❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤ | Elapsed Time: 0:00:09 ETA:   0:00:00\n",
      "Processed: 2999  97% |❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤ | Elapsed Time: 0:00:09 ETA:   0:00:00\n",
      "Processed: 3028  98% |❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤ | Elapsed Time: 0:00:09 ETA:   0:00:00\n",
      "Processed: 3049  98% |❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤ | Elapsed Time: 0:00:09 ETA:   0:00:00\n",
      "Processed: 3080  99% |❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤ | Elapsed Time: 0:00:09 ETA:   0:00:00\n",
      "Processed: 3088 100% |❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤| Elapsed Time: 0:00:09 Time:  0:00:09\n"
     ]
    }
   ],
   "source": [
    "# change to mfd2\n",
    "DICT_TYPE = 'mfd2'\n",
    "# revise the output file name\n",
    "OUT_CSV_PATH = 'mfd2_score.csv'\n",
    "\n",
    "emfd_score = score_docs(template_input, DICT_TYPE, PROB_MAP, SCORE_METHOD, OUT_METRICS, num_docs)\n",
    "emfd_score.to_csv(OUT_CSV_PATH, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed: 0   0% |                      | Elapsed Time: 0:00:00 ETA:  --:--:--\n",
      "Processed: 33   1% |                     | Elapsed Time: 0:00:00 ETA:   0:00:09\n",
      "Processed: 65   2% |                     | Elapsed Time: 0:00:00 ETA:   0:00:09\n",
      "Processed: 94   3% |                     | Elapsed Time: 0:00:00 ETA:   0:00:09\n",
      "Processed: 118   3% |                    | Elapsed Time: 0:00:00 ETA:   0:00:10\n",
      "Processed: 149   4% |                    | Elapsed Time: 0:00:00 ETA:   0:00:09\n",
      "Processed: 177   5% |❤                   | Elapsed Time: 0:00:00 ETA:   0:00:09\n",
      "Processed: 196   6% |❤                   | Elapsed Time: 0:00:00 ETA:   0:00:09\n",
      "Processed: 230   7% |❤                   | Elapsed Time: 0:00:00 ETA:   0:00:09\n",
      "Processed: 264   8% |❤                   | Elapsed Time: 0:00:00 ETA:   0:00:09\n",
      "Processed: 294   9% |❤                   | Elapsed Time: 0:00:00 ETA:   0:00:09\n",
      "Processed: 313  10% |❤❤                  | Elapsed Time: 0:00:01 ETA:   0:00:09\n",
      "Processed: 347  11% |❤❤                  | Elapsed Time: 0:00:01 ETA:   0:00:08\n",
      "Processed: 385  12% |❤❤                  | Elapsed Time: 0:00:01 ETA:   0:00:08\n",
      "Processed: 412  13% |❤❤                  | Elapsed Time: 0:00:01 ETA:   0:00:08\n",
      "Processed: 430  13% |❤❤                  | Elapsed Time: 0:00:01 ETA:   0:00:08\n",
      "Processed: 464  15% |❤❤❤                 | Elapsed Time: 0:00:01 ETA:   0:00:08\n",
      "Processed: 479  15% |❤❤❤                 | Elapsed Time: 0:00:01 ETA:   0:00:08\n",
      "Processed: 505  16% |❤❤❤                 | Elapsed Time: 0:00:01 ETA:   0:00:08\n",
      "Processed: 545  17% |❤❤❤                 | Elapsed Time: 0:00:01 ETA:   0:00:08\n",
      "Processed: 579  18% |❤❤❤                 | Elapsed Time: 0:00:01 ETA:   0:00:08\n",
      "Processed: 614  19% |❤❤❤                 | Elapsed Time: 0:00:02 ETA:   0:00:08\n",
      "Processed: 646  20% |❤❤❤❤                | Elapsed Time: 0:00:02 ETA:   0:00:08\n",
      "Processed: 665  21% |❤❤❤❤                | Elapsed Time: 0:00:02 ETA:   0:00:08\n",
      "Processed: 701  22% |❤❤❤❤                | Elapsed Time: 0:00:02 ETA:   0:00:07\n",
      "Processed: 733  23% |❤❤❤❤                | Elapsed Time: 0:00:02 ETA:   0:00:07\n",
      "Processed: 771  24% |❤❤❤❤                | Elapsed Time: 0:00:02 ETA:   0:00:07\n",
      "Processed: 805  26% |❤❤❤❤❤               | Elapsed Time: 0:00:02 ETA:   0:00:07\n",
      "Processed: 821  26% |❤❤❤❤❤               | Elapsed Time: 0:00:02 ETA:   0:00:07\n",
      "Processed: 856  27% |❤❤❤❤❤               | Elapsed Time: 0:00:02 ETA:   0:00:07\n",
      "Processed: 893  28% |❤❤❤❤❤               | Elapsed Time: 0:00:02 ETA:   0:00:07\n",
      "Processed: 930  30% |❤❤❤❤❤❤              | Elapsed Time: 0:00:03 ETA:   0:00:07\n",
      "Processed: 968  31% |❤❤❤❤❤❤              | Elapsed Time: 0:00:03 ETA:   0:00:06\n",
      "Processed: 1002  32% |❤❤❤❤❤❤             | Elapsed Time: 0:00:03 ETA:   0:00:06\n",
      "Processed: 1042  33% |❤❤❤❤❤❤             | Elapsed Time: 0:00:03 ETA:   0:00:06\n",
      "Processed: 1056  34% |❤❤❤❤❤❤             | Elapsed Time: 0:00:03 ETA:   0:00:06\n",
      "Processed: 1078  34% |❤❤❤❤❤❤             | Elapsed Time: 0:00:03 ETA:   0:00:06\n",
      "Processed: 1117  36% |❤❤❤❤❤❤             | Elapsed Time: 0:00:03 ETA:   0:00:06\n",
      "Processed: 1157  37% |❤❤❤❤❤❤❤            | Elapsed Time: 0:00:03 ETA:   0:00:06\n",
      "Processed: 1173  37% |❤❤❤❤❤❤❤            | Elapsed Time: 0:00:03 ETA:   0:00:06\n",
      "Processed: 1207  39% |❤❤❤❤❤❤❤            | Elapsed Time: 0:00:03 ETA:   0:00:06\n",
      "Processed: 1246  40% |❤❤❤❤❤❤❤            | Elapsed Time: 0:00:03 ETA:   0:00:05\n",
      "Processed: 1284  41% |❤❤❤❤❤❤❤            | Elapsed Time: 0:00:04 ETA:   0:00:05\n",
      "Processed: 1313  42% |❤❤❤❤❤❤❤❤           | Elapsed Time: 0:00:04 ETA:   0:00:05\n",
      "Processed: 1330  43% |❤❤❤❤❤❤❤❤           | Elapsed Time: 0:00:04 ETA:   0:00:05\n",
      "Processed: 1368  44% |❤❤❤❤❤❤❤❤           | Elapsed Time: 0:00:04 ETA:   0:00:05\n",
      "Processed: 1395  45% |❤❤❤❤❤❤❤❤           | Elapsed Time: 0:00:04 ETA:   0:00:05\n",
      "Processed: 1432  46% |❤❤❤❤❤❤❤❤           | Elapsed Time: 0:00:04 ETA:   0:00:05\n",
      "Processed: 1462  47% |❤❤❤❤❤❤❤❤           | Elapsed Time: 0:00:04 ETA:   0:00:05\n",
      "Processed: 1486  48% |❤❤❤❤❤❤❤❤❤          | Elapsed Time: 0:00:04 ETA:   0:00:05\n",
      "Processed: 1518  49% |❤❤❤❤❤❤❤❤❤          | Elapsed Time: 0:00:04 ETA:   0:00:05\n",
      "Processed: 1540  49% |❤❤❤❤❤❤❤❤❤          | Elapsed Time: 0:00:04 ETA:   0:00:05\n",
      "Processed: 1561  50% |❤❤❤❤❤❤❤❤❤          | Elapsed Time: 0:00:05 ETA:   0:00:04\n",
      "Processed: 1584  51% |❤❤❤❤❤❤❤❤❤          | Elapsed Time: 0:00:05 ETA:   0:00:04\n",
      "Processed: 1603  51% |❤❤❤❤❤❤❤❤❤          | Elapsed Time: 0:00:05 ETA:   0:00:04\n",
      "Processed: 1626  52% |❤❤❤❤❤❤❤❤❤❤         | Elapsed Time: 0:00:05 ETA:   0:00:04\n",
      "Processed: 1642  53% |❤❤❤❤❤❤❤❤❤❤         | Elapsed Time: 0:00:05 ETA:   0:00:04\n",
      "Processed: 1670  54% |❤❤❤❤❤❤❤❤❤❤         | Elapsed Time: 0:00:05 ETA:   0:00:04\n",
      "Processed: 1696  54% |❤❤❤❤❤❤❤❤❤❤         | Elapsed Time: 0:00:05 ETA:   0:00:04\n",
      "Processed: 1720  55% |❤❤❤❤❤❤❤❤❤❤         | Elapsed Time: 0:00:05 ETA:   0:00:04\n",
      "Processed: 1744  56% |❤❤❤❤❤❤❤❤❤❤         | Elapsed Time: 0:00:05 ETA:   0:00:04\n",
      "Processed: 1759  56% |❤❤❤❤❤❤❤❤❤❤         | Elapsed Time: 0:00:05 ETA:   0:00:04\n",
      "Processed: 1786  57% |❤❤❤❤❤❤❤❤❤❤         | Elapsed Time: 0:00:05 ETA:   0:00:04\n",
      "Processed: 1815  58% |❤❤❤❤❤❤❤❤❤❤❤        | Elapsed Time: 0:00:06 ETA:   0:00:04\n",
      "Processed: 1836  59% |❤❤❤❤❤❤❤❤❤❤❤        | Elapsed Time: 0:00:06 ETA:   0:00:04\n",
      "Processed: 1859  60% |❤❤❤❤❤❤❤❤❤❤❤        | Elapsed Time: 0:00:06 ETA:   0:00:04\n",
      "Processed: 1877  60% |❤❤❤❤❤❤❤❤❤❤❤        | Elapsed Time: 0:00:06 ETA:   0:00:04\n",
      "Processed: 1894  61% |❤❤❤❤❤❤❤❤❤❤❤        | Elapsed Time: 0:00:06 ETA:   0:00:04\n",
      "Processed: 1910  61% |❤❤❤❤❤❤❤❤❤❤❤        | Elapsed Time: 0:00:06 ETA:   0:00:04\n",
      "Processed: 1933  62% |❤❤❤❤❤❤❤❤❤❤❤        | Elapsed Time: 0:00:06 ETA:   0:00:03\n",
      "Processed: 1955  63% |❤❤❤❤❤❤❤❤❤❤❤❤       | Elapsed Time: 0:00:06 ETA:   0:00:03\n",
      "Processed: 1978  64% |❤❤❤❤❤❤❤❤❤❤❤❤       | Elapsed Time: 0:00:06 ETA:   0:00:03\n",
      "Processed: 1994  64% |❤❤❤❤❤❤❤❤❤❤❤❤       | Elapsed Time: 0:00:06 ETA:   0:00:03\n",
      "Processed: 2018  65% |❤❤❤❤❤❤❤❤❤❤❤❤       | Elapsed Time: 0:00:07 ETA:   0:00:03\n",
      "Processed: 2033  65% |❤❤❤❤❤❤❤❤❤❤❤❤       | Elapsed Time: 0:00:07 ETA:   0:00:03\n",
      "Processed: 2058  66% |❤❤❤❤❤❤❤❤❤❤❤❤       | Elapsed Time: 0:00:07 ETA:   0:00:03\n",
      "Processed: 2072  67% |❤❤❤❤❤❤❤❤❤❤❤❤       | Elapsed Time: 0:00:07 ETA:   0:00:03\n",
      "Processed: 2095  67% |❤❤❤❤❤❤❤❤❤❤❤❤       | Elapsed Time: 0:00:07 ETA:   0:00:03\n",
      "Processed: 2111  68% |❤❤❤❤❤❤❤❤❤❤❤❤       | Elapsed Time: 0:00:07 ETA:   0:00:03\n",
      "Processed: 2126  68% |❤❤❤❤❤❤❤❤❤❤❤❤❤      | Elapsed Time: 0:00:07 ETA:   0:00:03\n",
      "Processed: 2150  69% |❤❤❤❤❤❤❤❤❤❤❤❤❤      | Elapsed Time: 0:00:07 ETA:   0:00:03\n",
      "Processed: 2177  70% |❤❤❤❤❤❤❤❤❤❤❤❤❤      | Elapsed Time: 0:00:07 ETA:   0:00:03\n",
      "Processed: 2189  70% |❤❤❤❤❤❤❤❤❤❤❤❤❤      | Elapsed Time: 0:00:07 ETA:   0:00:03\n",
      "Processed: 2216  71% |❤❤❤❤❤❤❤❤❤❤❤❤❤      | Elapsed Time: 0:00:07 ETA:   0:00:03\n",
      "Processed: 2229  72% |❤❤❤❤❤❤❤❤❤❤❤❤❤      | Elapsed Time: 0:00:07 ETA:   0:00:03\n",
      "Processed: 2239  72% |❤❤❤❤❤❤❤❤❤❤❤❤❤      | Elapsed Time: 0:00:08 ETA:   0:00:03\n",
      "Processed: 2251  72% |❤❤❤❤❤❤❤❤❤❤❤❤❤      | Elapsed Time: 0:00:08 ETA:   0:00:03\n",
      "Processed: 2268  73% |❤❤❤❤❤❤❤❤❤❤❤❤❤      | Elapsed Time: 0:00:08 ETA:   0:00:02\n",
      "Processed: 2295  74% |❤❤❤❤❤❤❤❤❤❤❤❤❤❤     | Elapsed Time: 0:00:08 ETA:   0:00:02\n",
      "Processed: 2307  74% |❤❤❤❤❤❤❤❤❤❤❤❤❤❤     | Elapsed Time: 0:00:08 ETA:   0:00:02\n",
      "Processed: 2331  75% |❤❤❤❤❤❤❤❤❤❤❤❤❤❤     | Elapsed Time: 0:00:08 ETA:   0:00:02\n",
      "Processed: 2346  75% |❤❤❤❤❤❤❤❤❤❤❤❤❤❤     | Elapsed Time: 0:00:08 ETA:   0:00:02\n",
      "Processed: 2371  76% |❤❤❤❤❤❤❤❤❤❤❤❤❤❤     | Elapsed Time: 0:00:08 ETA:   0:00:02\n",
      "Processed: 2385  77% |❤❤❤❤❤❤❤❤❤❤❤❤❤❤     | Elapsed Time: 0:00:08 ETA:   0:00:02\n",
      "Processed: 2411  78% |❤❤❤❤❤❤❤❤❤❤❤❤❤❤     | Elapsed Time: 0:00:08 ETA:   0:00:02\n",
      "Processed: 2424  78% |❤❤❤❤❤❤❤❤❤❤❤❤❤❤     | Elapsed Time: 0:00:08 ETA:   0:00:02\n",
      "Processed: 2436  78% |❤❤❤❤❤❤❤❤❤❤❤❤❤❤     | Elapsed Time: 0:00:09 ETA:   0:00:02\n",
      "Processed: 2462  79% |❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤    | Elapsed Time: 0:00:09 ETA:   0:00:02\n",
      "Processed: 2488  80% |❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤    | Elapsed Time: 0:00:09 ETA:   0:00:02\n",
      "Processed: 2502  81% |❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤    | Elapsed Time: 0:00:09 ETA:   0:00:02\n",
      "Processed: 2531  81% |❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤    | Elapsed Time: 0:00:09 ETA:   0:00:02\n",
      "Processed: 2555  82% |❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤    | Elapsed Time: 0:00:09 ETA:   0:00:01\n",
      "Processed: 2580  83% |❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤    | Elapsed Time: 0:00:09 ETA:   0:00:01\n",
      "Processed: 2607  84% |❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤   | Elapsed Time: 0:00:09 ETA:   0:00:01\n",
      "Processed: 2619  84% |❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤   | Elapsed Time: 0:00:09 ETA:   0:00:01\n",
      "Processed: 2645  85% |❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤   | Elapsed Time: 0:00:09 ETA:   0:00:01\n",
      "Processed: 2677  86% |❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤   | Elapsed Time: 0:00:10 ETA:   0:00:01\n",
      "Processed: 2698  87% |❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤   | Elapsed Time: 0:00:10 ETA:   0:00:01\n",
      "Processed: 2719  88% |❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤   | Elapsed Time: 0:00:10 ETA:   0:00:01\n",
      "Processed: 2737  88% |❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤   | Elapsed Time: 0:00:10 ETA:   0:00:01\n",
      "Processed: 2758  89% |❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤   | Elapsed Time: 0:00:10 ETA:   0:00:01\n",
      "Processed: 2776  89% |❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤  | Elapsed Time: 0:00:10 ETA:   0:00:01\n",
      "Processed: 2793  90% |❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤  | Elapsed Time: 0:00:10 ETA:   0:00:01\n",
      "Processed: 2813  91% |❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤  | Elapsed Time: 0:00:10 ETA:   0:00:01\n",
      "Processed: 2833  91% |❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤  | Elapsed Time: 0:00:10 ETA:   0:00:00\n",
      "Processed: 2853  92% |❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤  | Elapsed Time: 0:00:10 ETA:   0:00:00\n",
      "Processed: 2872  93% |❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤  | Elapsed Time: 0:00:11 ETA:   0:00:00\n",
      "Processed: 2889  93% |❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤  | Elapsed Time: 0:00:11 ETA:   0:00:00\n",
      "Processed: 2908  94% |❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤  | Elapsed Time: 0:00:11 ETA:   0:00:00\n",
      "Processed: 2929  94% |❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤ | Elapsed Time: 0:00:11 ETA:   0:00:00\n",
      "Processed: 2950  95% |❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤ | Elapsed Time: 0:00:11 ETA:   0:00:00\n",
      "Processed: 2966  96% |❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤ | Elapsed Time: 0:00:11 ETA:   0:00:00\n",
      "Processed: 2981  96% |❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤ | Elapsed Time: 0:00:11 ETA:   0:00:00\n",
      "Processed: 3000  97% |❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤ | Elapsed Time: 0:00:11 ETA:   0:00:00\n",
      "Processed: 3021  97% |❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤ | Elapsed Time: 0:00:11 ETA:   0:00:00\n",
      "Processed: 3041  98% |❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤ | Elapsed Time: 0:00:11 ETA:   0:00:00\n",
      "Processed: 3060  99% |❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤ | Elapsed Time: 0:00:12 ETA:   0:00:00\n",
      "Processed: 3083  99% |❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤ | Elapsed Time: 0:00:12 ETA:   0:00:00\n",
      "Processed: 3088 100% |❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤| Elapsed Time: 0:00:12 Time:  0:00:12\n"
     ]
    }
   ],
   "source": [
    "# change to mfd for some extra reference\n",
    "DICT_TYPE = 'mfd'\n",
    "# revise the output file name\n",
    "OUT_CSV_PATH = 'mfd_score.csv'\n",
    "\n",
    "emfd_score = score_docs(template_input, DICT_TYPE, PROB_MAP, SCORE_METHOD, OUT_METRICS, num_docs)\n",
    "emfd_score.to_csv(OUT_CSV_PATH, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### appendix 1 \n",
    "# appendix1_vignette = pd.read_csv('/workspaces/llms_mft_multilingual/moral_vignette_original.csv')\n",
    "# appendix1_vignette = appendix1_vignette[['Scenario']]\n",
    "# appendix1_vignette.to_csv('/workspaces/llms_mft_multilingual/appendix1_vignette.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed: 0   0% |                      | Elapsed Time: 0:00:00 ETA:  --:--:--\n",
      "Processed: 17  18% |❤❤❤                  | Elapsed Time: 0:00:00 ETA:   0:00:00\n",
      "Processed: 34  37% |❤❤❤❤❤❤❤              | Elapsed Time: 0:00:00 ETA:   0:00:00\n",
      "Processed: 52  57% |❤❤❤❤❤❤❤❤❤❤❤❤         | Elapsed Time: 0:00:00 ETA:   0:00:00\n",
      "Processed: 71  78% |❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤     | Elapsed Time: 0:00:00 ETA:   0:00:00\n",
      "Processed: 90  98% |❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤ | Elapsed Time: 0:00:00 ETA:   0:00:00\n",
      "Processed: 91 100% |❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤| Elapsed Time: 0:00:00 Time:  0:00:00\n"
     ]
    }
   ],
   "source": [
    "template_input = pd.read_csv('/workspaces/llms_mft_multilingual/experiment1/appendix1_vignette.csv', header=None)\n",
    "num_docs = len(template_input)\n",
    "\n",
    "DICT_TYPE = 'emfd'\n",
    "PROB_MAP = 'all'\n",
    "SCORE_METHOD = 'bow'\n",
    "OUT_METRICS = 'sentiment'\n",
    "OUT_CSV_PATH = 'appendix1_vignette_emfd.csv'\n",
    "\n",
    "emfd_score = score_docs(template_input, DICT_TYPE, PROB_MAP, SCORE_METHOD, OUT_METRICS, num_docs)\n",
    "emfd_score.to_csv(OUT_CSV_PATH, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed: 0   0% |                      | Elapsed Time: 0:00:00 ETA:  --:--:--\n",
      "Processed: 15  16% |❤❤❤                  | Elapsed Time: 0:00:00 ETA:   0:00:00\n",
      "Processed: 32  35% |❤❤❤❤❤❤❤              | Elapsed Time: 0:00:00 ETA:   0:00:00\n",
      "Processed: 50  54% |❤❤❤❤❤❤❤❤❤❤❤          | Elapsed Time: 0:00:00 ETA:   0:00:00\n",
      "Processed: 68  74% |❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤      | Elapsed Time: 0:00:00 ETA:   0:00:00\n",
      "Processed: 85  93% |❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤  | Elapsed Time: 0:00:00 ETA:   0:00:00\n",
      "Processed: 91 100% |❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤| Elapsed Time: 0:00:00 Time:  0:00:00\n"
     ]
    }
   ],
   "source": [
    "DICT_TYPE = 'mfd2'\n",
    "OUT_CSV_PATH = 'appendix1_vignette_mfd2.csv'\n",
    "\n",
    "emfd_score = score_docs(template_input, DICT_TYPE, PROB_MAP, SCORE_METHOD, OUT_METRICS, num_docs)\n",
    "emfd_score.to_csv(OUT_CSV_PATH, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('/workspaces/llms_mft_multilingual/benchmarkset.csv', dtype=str)\n",
    "benchmark_dict = pd.read_csv('/workspaces/llms_mft_multilingual/experiment1/benchmark_dict.csv')\n",
    "emfd_score = pd.read_csv('/workspaces/llms_mft_multilingual/experiment1/emfd_score.csv')\n",
    "mfd2_score = pd.read_csv('/workspaces/llms_mft_multilingual/experiment1/mfd2_score.csv')\n",
    "mfd_score = pd.read_csv('/workspaces/llms_mft_multilingual/experiment1/mfd_score.csv')\n",
    "\n",
    "appendix1_vignette = pd.read_csv('/workspaces/llms_mft_multilingual/experiment1/appendix1_vignette.csv')\n",
    "vignette_emfd = pd.read_csv('/workspaces/llms_mft_multilingual/experiment1/appendix1_vignette_emfd.csv')\n",
    "vignette_mfd2 = pd.read_csv('/workspaces/llms_mft_multilingual/experiment1/appendix1_vignette_mfd2.csv')\n",
    "\n",
    "# print(benchmark_dict.shape)\n",
    "# print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "emfd_score = emfd_score.drop(index=0).reset_index(drop=True)\n",
    "result_emfd = pd.concat([benchmark_dict, emfd_score], axis=1)\n",
    "\n",
    "mfd2_score = mfd2_score.drop(index=0).reset_index(drop=True)\n",
    "result_mfd2 = pd.concat([benchmark_dict, mfd2_score], axis=1)\n",
    "\n",
    "mfd_score = mfd_score.drop(index=0).reset_index(drop=True)\n",
    "result_mfd = pd.concat([benchmark_dict, mfd_score], axis=1)\n",
    "\n",
    "#### appendix 1 ####\n",
    "vignette_emfd = vignette_emfd.drop(index=0).reset_index(drop=True)\n",
    "result_vignette_emfd = pd.concat([appendix1_vignette, vignette_emfd], axis=1)\n",
    "\n",
    "vignette_mfd2 = vignette_mfd2.drop(index=0).reset_index(drop=True)\n",
    "result_vignette_mfd2 = pd.concat([appendix1_vignette, vignette_mfd2], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_emfd.to_csv('/home/misinfo/turing_dso_misinfo/llms_mft_multilingual/paper_linked_final/experiment_machine_translation/result_raw/result_emfd.csv', index=False)\n",
    "result_mfd2.to_csv('/home/misinfo/turing_dso_misinfo/llms_mft_multilingual/paper_linked_final/experiment_machine_translation/result_raw/result_mfd2.csv', index=False)\n",
    "result_mfd.to_csv('/home/misinfo/turing_dso_misinfo/llms_mft_multilingual/paper_linked_final/experiment_machine_translation/result_raw/result_mfd.csv', index=False)\n",
    "result_vignette_emfd.to_csv('/home/misinfo/turing_dso_misinfo/llms_mft_multilingual/paper_linked_final/experiment_machine_translation/result_raw/result_vignette_emfd.csv', index=False)\n",
    "result_vignette_mfd2.to_csv('/home/misinfo/turing_dso_misinfo/llms_mft_multilingual/paper_linked_final/experiment_machine_translation/result_raw/result_vignette_mfd2.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## E 1.2 Word Embeddings: FrameAxis and MoralStrength"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E1.2.1 FrameAxis with MFD, MFD2, and eMFD dictionary\n",
    "\n",
    "I used command line directly from the terminal to run the FrameAxis code.\n",
    "\n",
    "GitHub page [FrameAxis](https://github.com/negar-mokhberian/Moral_Foundation_FrameAxis).\n",
    "* the github repo is not maintained anymore, you have to revise the following change to make it work in the terminal. \n",
    "* after clone the repo, you need to change some files to make it work\n",
    "    * the `main.py`, line 50, remove the separator argument, change it to `data = pd.read_csv(IN_PATH)`\n",
    "    * the `frameAxis.py`, after line 19, add two lines `self.axes, categories = self._get_emfd_axes(words_df)` and `print('axes names: ', categories)` to fix the category key error when using emfd dictionary, thanks to [\n",
    "edoardo-lucini](https://github.com/negar-mokhberian/Moral_Foundation_FrameAxis/pull/8/files)\n",
    "\n",
    "steps to use the work in your terminal \n",
    "1. clone the repo to machine `git clone https://github.com/username/repository.git`\n",
    "2. `pip` install necessary packages if you have not done so, e.g., `nltk`, `genism`\n",
    "3. change the `main.py` and `frameAxis.py` as mentioned above\n",
    "4. run the command line `python main.py --docs_colname google_trans_en --input_file /workspaces/llms_mft_multilingual/benchmarkset.csv --output_file frameaxis_mfd2.csv --dict mfd2`\n",
    "    1. `main.py` specifies the file path\n",
    "    2. `frameaxis_mfd2.csv`  change to the output file name\n",
    "    3. `mfd2` change to the dictionary you want to use, e.g., `mfd`, `mfd2`, `emfd`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E 1.2.2 MoralStrength\n",
    "\n",
    "- [original paper](https://arxiv.org/abs/1904.08314)\n",
    "- Github repository: [moralstrength](https://github.com/oaraque/moral-foundations)\n",
    "- Workshop materials: [moralstrength - IC2S2 2023](https://github.com/oaraque/human-values-tutorial-ic2s2-2023/blob/main/MoralValues/Moral-Value-Estimation.ipynb)\n",
    "- python package: [moralstrength 0.2.13](https://pypi.org/project/moralstrength/)\n",
    "- note: experiment is done seperately on a remote server as my local machine does not have enough memory to run the below code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # %pip install moralstrength \n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "# from scipy.sparse import hstack\n",
    "# from sklearn.preprocessing import minmax_scale\n",
    "# from sklearn.metrics import classification_report, confusion_matrix\n",
    "# from sklearn.model_selection import cross_val_predict\n",
    "# from sklearn.linear_model import SGDClassifier\n",
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# from tqdm.notebook import tqdm\n",
    "\n",
    "# import moralstrength\n",
    "# from moralstrength.moralstrength import estimate_morals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #### load benchmark data ####\n",
    "# bm = pd.read_csv('/data/scro4316/thesis/paper3/BM.csv')\n",
    "\n",
    "# #### load MT-En data and preprocess ####\n",
    "# mt_twitter = pd.read_csv('/data/scro4316/thesis/paper3/MT-EN-Twitter.csv')\n",
    "# mt_reddit = pd.read_csv('/data/scro4316/thesis/paper3/MT-EN-Reddit.csv')\n",
    "\n",
    "# # map rule should be the same with MoralStrength labels : care, fairness, authority, loyalty, purity\n",
    "# mapping_reddit = {\n",
    "#     \"Non-Moral\": \"non-moral\",\n",
    "#     \"Care\": \"care\",\n",
    "#     \"Fairness\": \"fairness\",\n",
    "#     \"Authority\": \"authority\",\n",
    "#     \"Loyalty\": \"loyalty\",\n",
    "#     \"Purity\": \"purity\",\n",
    "#     \"Thin Morality\": \"non-moral\"\n",
    "# }\n",
    "\n",
    "# mapping_tweet = {\n",
    "#     \"non-moral\": \"non-moral\", \n",
    "#     \"care\": \"care\",\n",
    "#     \"harm\": \"care\",\n",
    "#     \"fairness\":  \"fairness\",\n",
    "#     \"cheating\":  \"fairness\",\n",
    "#     \"authority\": \"authority\",\n",
    "#     \"subversion\": \"authority\",\n",
    "#     \"loyalty\": \"loyalty\",\n",
    "#     \"betrayal\": \"loyalty\",\n",
    "#     \"purity\": \"purity\",\n",
    "#     \"degradation\": \"purity\",\n",
    "#     \"nm\": \"non-moral\"\n",
    "# }\n",
    "\n",
    "# mt_twitter[\"label\"] = mt_twitter[\"annotation\"].map(mapping_tweet)\n",
    "# mt_reddit['label'] = mt_reddit['annotation'].map(mapping_reddit)\n",
    "\n",
    "# mt_en = pd.concat([mt_twitter[['txt_clean','label']], mt_reddit[['txt_clean','label']]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #### Trainin with all dataset ####\n",
    "# # 3.1 preprocess\n",
    "# texts_preprocessed = list()\n",
    "# for text in tqdm(moralstrength.nlp_reduced.pipe(mt_en[\"txt_clean\"])):\n",
    "#     texts_preprocessed.append(text)\n",
    "\n",
    "# # 3.2 Extract TF-IDF features from the MT-EN dataset\n",
    "# texts = list()\n",
    "# for doc in texts_preprocessed:\n",
    "#     text_i = list()\n",
    "#     for token in doc:\n",
    "#         text_i.append(token.lemma_)\n",
    "#     texts.append(\" \".join(text_i))\n",
    "\n",
    "# unigram = CountVectorizer(max_features=10000)\n",
    "# unigram_features = unigram.fit_transform(texts)\n",
    "\n",
    "# # 3.3 Extract moral features from the MT-EN dataset with MoralStrength\n",
    "# result = estimate_morals(mt_en[\"txt_clean\"], process=True)\n",
    "# moralstrength_features = minmax_scale((result.fillna(5.0) - 5).values)\n",
    "\n",
    "# # 3.4 combine the features\n",
    "# combined_features = hstack([unigram_features, moralstrength_features])\n",
    "\n",
    "# # 3.5 Train the classifier on the entire MT-EN dataset with combined features \n",
    "# def get_classifier():\n",
    "#     return SGDClassifier(loss=\"hinge\", random_state=42)\n",
    "\n",
    "# classifier = get_classifier()\n",
    "# classifier.fit(combined_features, mt_en[\"label\"])\n",
    "\n",
    "# # 3.6 save the trained classifier \n",
    "# # joblib.dump(classifier, 'trained_classifier.pkl')\n",
    "\n",
    "\n",
    "# ########################################################################################################################\n",
    "# #### Trainin with moral-relevant dataset (because our BM does not have non-moral records) ####\n",
    "\n",
    "# mt_en_2 = mt_en[mt_en['label'] != 'non-moral']\n",
    "\n",
    "# # 3.1 preprocess\n",
    "# texts_preprocessed = list()\n",
    "# for text in tqdm(moralstrength.nlp_reduced.pipe(mt_en_2[\"txt_clean\"])):\n",
    "#     texts_preprocessed.append(text)\n",
    "\n",
    "# # 3.2 Extract TF-IDF features from the MT-EN dataset\n",
    "# texts = list()\n",
    "# for doc in texts_preprocessed:\n",
    "#     text_i = list()\n",
    "#     for token in doc:\n",
    "#         text_i.append(token.lemma_)\n",
    "#     texts.append(\" \".join(text_i))\n",
    "\n",
    "# unigram = CountVectorizer(max_features=10000)\n",
    "# unigram_features = unigram.fit_transform(texts)\n",
    "\n",
    "# # 3.3 Extract moral features from the MT-EN dataset with MoralStrength\n",
    "# result = estimate_morals(mt_en_2[\"txt_clean\"], process=True)\n",
    "# moralstrength_features = minmax_scale((result.fillna(5.0) - 5).values)\n",
    "\n",
    "# # 3.4 combine the features\n",
    "# combined_features = hstack([unigram_features, moralstrength_features])\n",
    "\n",
    "# # 3.5 Train the classifier on the entire MT-EN dataset with combined features \n",
    "# def get_classifier():\n",
    "#     return SGDClassifier(loss=\"hinge\", random_state=42)\n",
    "\n",
    "# classifier2 = get_classifier()\n",
    "# classifier2.fit(combined_features, mt_en_2[\"label\"])\n",
    "\n",
    "# # 3.6 save the trained classifier \n",
    "# # joblib.dump(classifier, 'trained_classifier.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 3.1 Preprocess the BM dataset\n",
    "# new_texts_preprocessed = list()\n",
    "# for text in tqdm(moralstrength.nlp_reduced.pipe(bm[\"google_trans_en\"])):\n",
    "#     new_texts_preprocessed.append(text)\n",
    "\n",
    "# # 3.2 Extract features from the new dataset\n",
    "# new_texts = list()\n",
    "# for doc in new_texts_preprocessed:\n",
    "#     text_i = list()\n",
    "#     for token in doc:\n",
    "#         text_i.append(token.lemma_)\n",
    "#     new_texts.append(\" \".join(text_i))\n",
    "\n",
    "# # Use the same CountVectorizer as before\n",
    "# new_unigram_features = unigram.transform(new_texts)\n",
    "\n",
    "# # Assuming you have the moral strength features for the new dataset in a variable called new_result\n",
    "# new_result = estimate_morals(bm[\"google_trans_en\"], process=True)\n",
    "# new_moralstrength_features = minmax_scale((new_result.fillna(5.0) - 5).values)\n",
    "\n",
    "# # Combine the features\n",
    "# new_combined_features = hstack([new_unigram_features, new_moralstrength_features])\n",
    "\n",
    "# # Step 4: Predict the labels for the new dataset\n",
    "# new_predictions = classifier.predict(new_combined_features)\n",
    "# # Predict the labels for the new dataset with classifier2\n",
    "# new_predictions2 = classifier2.predict(new_combined_features)\n",
    "\n",
    "\n",
    "# # Add the predictions to the new DataFrame\n",
    "# bm['moral_strength_sup_svm'] = new_predictions\n",
    "# bm['moral_strength_sup_svm2'] = new_predictions2\n",
    "\n",
    "# # Display the DataFrame with the predictions\n",
    "# # print(bm['moral_strength_sup_svm'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #### Unsupervised moral strength estimation ####\n",
    "# unsup_result = estimate_morals(bm[\"google_trans_en\"], process=True)\n",
    "# unsup_predictions = np.abs(result - 5).idxmax(axis=1).fillna(\"non-moral\")\n",
    "# bm['moral_strength_unsup'] = unsup_predictions\n",
    "\n",
    "# # save the predictions\n",
    "# bm.to_csv('bm_moral_strength.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## E 1.3 Deep Learning Language Models: Mformer and MoralBert\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E 1.3.1 Language Models: Mformers\n",
    "- source\n",
    "    - MFormer [Github Repo](https://github.com/joshnguyen99/moral_axes)\n",
    "    - MFormer [Hugging Face](https://huggingface.co/joshnguyen)\n",
    "- notes\n",
    "    - it requires some memories on your local machine\n",
    "    - result saved in `/workspaces/llms_mft_multilingual/experiment1/result_mformer.csv`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import re\n",
    "# import torch\n",
    "# import tweetnlp\n",
    "# import os\n",
    "# import csv # For reading the CSV file\n",
    "# from tqdm import tqdm  # For progress bar\n",
    "# from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "\n",
    "# df = pd.read_csv('/data/scro4316/thesis/paper3/benchmarkset.csv', dtype=str)\n",
    "# # 1. convert the dataframe to a list of dictionaries prepare for the model inference\n",
    "# instances = df.apply(lambda row: {\"id\": row['text'], \"text\": row['google_trans_en']}, axis=1).tolist()\n",
    "# print(len(instances))\n",
    "\n",
    "# # 3. change the working directory to the location where you want to save the results\n",
    "# os.chdir('/data/scro4316/thesis/paper3')\n",
    "\n",
    "\n",
    "# # Define foundation names\n",
    "# FOUNDATIONS = [\"authority\",\"care\", \"fairness\", \"loyalty\", \"sanctity\"]\n",
    "# # Load models and tokenizers for each foundation\n",
    "# models = {}\n",
    "# tokenizers = {}\n",
    "# device = 1 if torch.cuda.is_available() else -1\n",
    "\n",
    "# for foundation in FOUNDATIONS:\n",
    "#     model_name = f\"joshnguyen/mformer-{foundation}\"\n",
    "#     tokenizers[foundation] = AutoTokenizer.from_pretrained(model_name)\n",
    "#     models[foundation] = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "#     models[foundation].to(device)  # Move model to the desired device\n",
    "\n",
    "# # Process all instances in a single batch\n",
    "# texts = [item[\"text\"] for item in instances]\n",
    "# ids = [item[\"id\"] for item in instances]\n",
    "\n",
    "# # Initialize results\n",
    "# results = [{\"id\": id} for id in ids]\n",
    "\n",
    "# for foundation in FOUNDATIONS:\n",
    "#     tokenizer = tokenizers[foundation]\n",
    "#     model = models[foundation]\n",
    "    \n",
    "#     # Encode the batch\n",
    "#     inputs = tokenizer(\n",
    "#         texts,\n",
    "#         padding=True,\n",
    "#         truncation=True,\n",
    "#         return_tensors='pt'\n",
    "#     ).to(model.device)\n",
    "    \n",
    "#     # Forward pass\n",
    "#     model.eval()\n",
    "#     with torch.no_grad():\n",
    "#         outputs = model(**inputs)\n",
    "    \n",
    "#     # Calculate class probabilities\n",
    "#     probs = torch.softmax(outputs.logits, dim=1)\n",
    "#     probs = probs[:, 1]\n",
    "#     probs = probs.detach().cpu().numpy()\n",
    "    \n",
    "#     # Add probabilities to results\n",
    "#     for result, prob in zip(results, probs):\n",
    "#         result[f\"{foundation}_prob\"] = prob\n",
    "\n",
    "# # Save results to CSV file\n",
    "# csv_file = \"result_mformer.csv\"\n",
    "# with open(csv_file, mode='w', newline='') as file:\n",
    "#     writer = csv.DictWriter(file, fieldnames=[\"id\"] + [f\"{foundation}_prob\" for foundation in FOUNDATIONS])\n",
    "#     writer.writeheader()\n",
    "#     writer.writerows(results)\n",
    "\n",
    "# print(f\"Results saved to {csv_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E 1.3.2 MoralBert\n",
    "- [Github](https://github.com/vjosapreniqi/MoralBERT)\n",
    "- [Hugging Face](https://huggingface.co/vjosap)\n",
    "- notes\n",
    "    1. estimation is on brain-server `e1_3_2.py`\n",
    "    2. results saved `/home/misinfo/turing_dso_misinfo/llms_mft_multilingual/paper_linked_final/experiment_machine_translation/result_raw/result_moralbert.csv`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Formatting and Cleaning Raw Results\n",
    "1. all raw results are saved in the `result_raw` folder\n",
    "2. cleaned results are saved in the `result` folder with time stamps\n",
    "3. cleaned results should be ready for analysis and visualization, formatted as ``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source_label\n",
      "care    1035\n",
      "auth     627\n",
      "loya     613\n",
      "fair     524\n",
      "sanc     288\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# load the BM dataset to store all MT results\n",
    "result_mt = pd.read_csv('/home/misinfo/turing_dso_misinfo/llms_mft_multilingual/paper_linked_final/datasets/BM.csv')\n",
    "# keep necessary columns\n",
    "result_mt = result_mt[['text', 'google_trans_en', 'source', 'source_dataset', 'source_label']]\n",
    "# check the ground truth labels\n",
    "print(result_mt['source_label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>google_trans_en</th>\n",
       "      <th>source</th>\n",
       "      <th>source_dataset</th>\n",
       "      <th>source_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>在地铁上看到十几岁的男孩在嘲笑截肢者。</td>\n",
       "      <td>Teenage boys were seen laughing at amputees on...</td>\n",
       "      <td>vignette</td>\n",
       "      <td>BM_MFV</td>\n",
       "      <td>care</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>女孩在学校话剧中嘲笑另一名忘词的同学。</td>\n",
       "      <td>A girl mocks another classmate for forgetting ...</td>\n",
       "      <td>vignette</td>\n",
       "      <td>BM_MFV</td>\n",
       "      <td>care</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>女人大声评论另一名女子的牛仔裤有多显胖。</td>\n",
       "      <td>A woman makes a loud comment about how fat ano...</td>\n",
       "      <td>vignette</td>\n",
       "      <td>BM_MFV</td>\n",
       "      <td>care</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>男人在看到相亲对象后立刻取消约会。</td>\n",
       "      <td>Men immediately cancel the date after seeing t...</td>\n",
       "      <td>vignette</td>\n",
       "      <td>BM_MFV</td>\n",
       "      <td>care</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>男孩告诉女人，她看起来就像她超重的斗牛犬。</td>\n",
       "      <td>Boy tells woman she looks just like her overwe...</td>\n",
       "      <td>vignette</td>\n",
       "      <td>BM_MFV</td>\n",
       "      <td>care</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    text                                    google_trans_en  \\\n",
       "0    在地铁上看到十几岁的男孩在嘲笑截肢者。  Teenage boys were seen laughing at amputees on...   \n",
       "1    女孩在学校话剧中嘲笑另一名忘词的同学。  A girl mocks another classmate for forgetting ...   \n",
       "2   女人大声评论另一名女子的牛仔裤有多显胖。  A woman makes a loud comment about how fat ano...   \n",
       "3      男人在看到相亲对象后立刻取消约会。  Men immediately cancel the date after seeing t...   \n",
       "4  男孩告诉女人，她看起来就像她超重的斗牛犬。  Boy tells woman she looks just like her overwe...   \n",
       "\n",
       "     source source_dataset source_label  \n",
       "0  vignette         BM_MFV         care  \n",
       "1  vignette         BM_MFV         care  \n",
       "2  vignette         BM_MFV         care  \n",
       "3  vignette         BM_MFV         care  \n",
       "4  vignette         BM_MFV         care  "
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_mt.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean 1. Word Frequency - MFD, MFD2, and eMFD dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Function to determine the moral value for the text, based on the highest score (prob or freq) of the dictionary results\n",
    "def determine_score(row):\n",
    "    values = row[columns_to_check]\n",
    "    if values.max() == 0:\n",
    "        return \"non_moral\"\n",
    "    else:\n",
    "        max_value = values.max()\n",
    "        max_columns = [col for col in columns_to_check if row[col] == max_value]\n",
    "        return ', '.join(max_columns)\n",
    "\n",
    "# decide the score label for each text \n",
    "columns_to_check = ['care', 'auth', 'fair', 'loya', 'sanc']\n",
    "\n",
    "\n",
    "# read the file \n",
    "df_emfd = pd.read_csv('/home/misinfo/turing_dso_misinfo/llms_mft_multilingual/paper_linked_final/experiment_machine_translation/result_raw/result_emfd.csv')\n",
    "df_mfd2 = pd.read_csv('/home/misinfo/turing_dso_misinfo/llms_mft_multilingual/paper_linked_final/experiment_machine_translation/result_raw/result_mfd2.csv')\n",
    "df_mfd = pd.read_csv('/home/misinfo/turing_dso_misinfo/llms_mft_multilingual/paper_linked_final/experiment_machine_translation/result_raw/result_mfd.csv')\n",
    "\n",
    "#### clean mfd based on its format #####\n",
    "df_mfd = pd.read_csv('/home/misinfo/turing_dso_misinfo/llms_mft_multilingual/paper_linked_final/experiment_machine_translation/result_raw/result_mfd.csv')\n",
    "\n",
    "df_mfd['care'] = df_mfd['care.vice'] + df_mfd['care.virtue']\n",
    "df_mfd['fair'] = df_mfd['fairness.vice'] + df_mfd['fairness.virtue']\n",
    "df_mfd['loya'] = df_mfd['loyalty.vice'] + df_mfd['loyalty.virtue']\n",
    "df_mfd['auth'] = df_mfd['authority.vice'] + df_mfd['authority.virtue']\n",
    "df_mfd['sanc'] = df_mfd['sanctity.vice'] + df_mfd['sanctity.virtue']\n",
    "\n",
    "# only keep necessary columns \n",
    "df_mfd = df_mfd[['google_trans_en', 'care', 'fair', 'loya', 'auth','sanc']]\n",
    "\n",
    "\n",
    "#### clean mfd2 based on its format #####\n",
    "df_mfd2 = pd.read_csv('/home/misinfo/turing_dso_misinfo/llms_mft_multilingual/paper_linked_final/experiment_machine_translation/result_raw/result_mfd2.csv')\n",
    "\n",
    "df_mfd2['care'] = df_mfd2['care.vice'] + df_mfd2['care.virtue']\n",
    "df_mfd2['fair'] = df_mfd2['fairness.vice'] + df_mfd2['fairness.virtue']\n",
    "df_mfd2['loya'] = df_mfd2['loyalty.vice'] + df_mfd2['loyalty.virtue']\n",
    "df_mfd2['auth'] = df_mfd2['authority.vice'] + df_mfd2['authority.virtue']\n",
    "df_mfd2['sanc'] = df_mfd2['sanctity.vice'] + df_mfd2['sanctity.virtue']\n",
    "\n",
    "# only keep necessary columns \n",
    "df_mfd2 = df_mfd2[['google_trans_en', 'care', 'fair', 'loya', 'auth','sanc']]\n",
    "\n",
    "\n",
    "#### clean emfd based on its format #####\n",
    "df_emfd = df_emfd[['google_trans_en', 'care_p', 'fairness_p', 'loyalty_p', 'authority_p','sanctity_p']]\n",
    "# rename the column to align with other results\n",
    "new_column_names_emfd = {\n",
    "    'google_trans_en': 'google_trans_en',\n",
    "    'care_p': 'care',\n",
    "    'fairness_p': 'fair',\n",
    "    'loyalty_p': 'loya',\n",
    "    'authority_p': 'auth',\n",
    "    'sanctity_p': 'sanc'\n",
    "}\n",
    "df_emfd = df_emfd.rename(columns=new_column_names_emfd)\n",
    "\n",
    "\n",
    "# apply the function to the dataframe and add it to the result dataframe\n",
    "result_mt['freq_mfd'] = df_mfd.apply(determine_score, axis=1)\n",
    "result_mt['freq_mfd2'] = df_mfd2.apply(determine_score, axis=1)\n",
    "result_mt['freq_emfd'] = df_emfd.apply(determine_score, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>google_trans_en</th>\n",
       "      <th>source</th>\n",
       "      <th>source_dataset</th>\n",
       "      <th>source_label</th>\n",
       "      <th>freq_mfd</th>\n",
       "      <th>freq_mfd2</th>\n",
       "      <th>freq_emfd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>在地铁上看到十几岁的男孩在嘲笑截肢者。</td>\n",
       "      <td>Teenage boys were seen laughing at amputees on...</td>\n",
       "      <td>vignette</td>\n",
       "      <td>BM_MFV</td>\n",
       "      <td>care</td>\n",
       "      <td>non_moral</td>\n",
       "      <td>non_moral</td>\n",
       "      <td>care</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>女孩在学校话剧中嘲笑另一名忘词的同学。</td>\n",
       "      <td>A girl mocks another classmate for forgetting ...</td>\n",
       "      <td>vignette</td>\n",
       "      <td>BM_MFV</td>\n",
       "      <td>care</td>\n",
       "      <td>non_moral</td>\n",
       "      <td>non_moral</td>\n",
       "      <td>sanc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>女人大声评论另一名女子的牛仔裤有多显胖。</td>\n",
       "      <td>A woman makes a loud comment about how fat ano...</td>\n",
       "      <td>vignette</td>\n",
       "      <td>BM_MFV</td>\n",
       "      <td>care</td>\n",
       "      <td>non_moral</td>\n",
       "      <td>non_moral</td>\n",
       "      <td>care</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>男人在看到相亲对象后立刻取消约会。</td>\n",
       "      <td>Men immediately cancel the date after seeing t...</td>\n",
       "      <td>vignette</td>\n",
       "      <td>BM_MFV</td>\n",
       "      <td>care</td>\n",
       "      <td>non_moral</td>\n",
       "      <td>non_moral</td>\n",
       "      <td>fair</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>男孩告诉女人，她看起来就像她超重的斗牛犬。</td>\n",
       "      <td>Boy tells woman she looks just like her overwe...</td>\n",
       "      <td>vignette</td>\n",
       "      <td>BM_MFV</td>\n",
       "      <td>care</td>\n",
       "      <td>non_moral</td>\n",
       "      <td>non_moral</td>\n",
       "      <td>care</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    text                                    google_trans_en  \\\n",
       "0    在地铁上看到十几岁的男孩在嘲笑截肢者。  Teenage boys were seen laughing at amputees on...   \n",
       "1    女孩在学校话剧中嘲笑另一名忘词的同学。  A girl mocks another classmate for forgetting ...   \n",
       "2   女人大声评论另一名女子的牛仔裤有多显胖。  A woman makes a loud comment about how fat ano...   \n",
       "3      男人在看到相亲对象后立刻取消约会。  Men immediately cancel the date after seeing t...   \n",
       "4  男孩告诉女人，她看起来就像她超重的斗牛犬。  Boy tells woman she looks just like her overwe...   \n",
       "\n",
       "     source source_dataset source_label   freq_mfd  freq_mfd2 freq_emfd  \n",
       "0  vignette         BM_MFV         care  non_moral  non_moral      care  \n",
       "1  vignette         BM_MFV         care  non_moral  non_moral      sanc  \n",
       "2  vignette         BM_MFV         care  non_moral  non_moral      care  \n",
       "3  vignette         BM_MFV         care  non_moral  non_moral      fair  \n",
       "4  vignette         BM_MFV         care  non_moral  non_moral      care  "
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_mt.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean 2. Word Embeddings: FrameAxis and MoralStrength"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Function to determine the moral value for the text, based on the highest score (prob or freq) of the dictionary results\n",
    "def determine_score(row):\n",
    "    values = row[columns_to_check]\n",
    "    if values.max() == 0:\n",
    "        return \"non_moral\"\n",
    "    else:\n",
    "        max_value = values.max()\n",
    "        max_columns = [col for col in columns_to_check if row[col] == max_value]\n",
    "        return ', '.join(max_columns)\n",
    "\n",
    "# decide the score label for each text \n",
    "columns_to_check = ['care', 'auth', 'fair', 'loya', 'sanc']\n",
    "\n",
    "##### frameaxis_mfd #####\n",
    "frameaxis_mfd = pd.read_csv('/home/misinfo/turing_dso_misinfo/llms_mft_multilingual/paper_linked_final/experiment_machine_translation/result_raw/result_frameaxis_mfd.csv')\n",
    "frameaxis_mfd = frameaxis_mfd[['google_trans_en', 'intensity_harm', 'intensity_authority','intensity_fairness', 'intensity_ingroup', 'intensity_purity']]\n",
    "\n",
    "# only use intensity, intensity columns are results from FrameAxis, other columns are from the previous dictionaries\n",
    "new_column_names_mfd = {\n",
    "    'google_trans_en': 'google_trans_en',\n",
    "    'intensity_harm': 'care',\n",
    "    'intensity_fairness': 'fair',\n",
    "    'intensity_ingroup': 'loya',\n",
    "    'intensity_authority': 'auth',\n",
    "    'intensity_purity': 'sanc'\n",
    "}\n",
    "frameaxis_mfd = frameaxis_mfd.rename(columns=new_column_names_mfd)\n",
    "\n",
    "##### frameaxis_mfd2 #####\n",
    "frameaxis_mfd2 = pd.read_csv('/home/misinfo/turing_dso_misinfo/llms_mft_multilingual/paper_linked_final/experiment_machine_translation/result_raw/result_frameaxis_mfd2.csv')\n",
    "frameaxis_mfd2 = frameaxis_mfd2[['google_trans_en', 'intensity_care', 'intensity_authority','intensity_fairness', 'intensity_loyalty', 'intensity_sanctity']]\n",
    "new_column_names_mfd2 = {\n",
    "    'google_trans_en': 'google_trans_en',\n",
    "    'intensity_care': 'care',\n",
    "    'intensity_fairness': 'fair',\n",
    "    'intensity_loyalty': 'loya',\n",
    "    'intensity_authority': 'auth',\n",
    "    'intensity_sanctity': 'sanc'\n",
    "}\n",
    "# same column labels as mfd\n",
    "frameaxis_mfd2 = frameaxis_mfd2.rename(columns=new_column_names_mfd2) \n",
    "\n",
    "\n",
    "##### frameaxis_emfd #####\n",
    "frameaxis_emfd = pd.read_csv('/home/misinfo/turing_dso_misinfo/llms_mft_multilingual/paper_linked_final/experiment_machine_translation/result_raw/result_frameaxis_emfd.csv')\n",
    "frameaxis_emfd = frameaxis_emfd[['google_trans_en', 'intensity_care', 'intensity_authority','intensity_fairness', 'intensity_loyalty', 'intensity_sanctity']]\n",
    "\n",
    "new_column_names_frameaxis = {\n",
    "    'google_trans_en': 'google_trans_en',\n",
    "    'intensity_care': 'care',\n",
    "    'intensity_fairness': 'fair',\n",
    "    'intensity_loyalty': 'loya',\n",
    "    'intensity_authority': 'auth',\n",
    "    'intensity_sanctity': 'sanc'\n",
    "}\n",
    "frameaxis_emfd = frameaxis_emfd.rename(columns=new_column_names_frameaxis)\n",
    "\n",
    "### save the result to the same dataframe\n",
    "result_mt['frameaxis_mfd'] = frameaxis_mfd.apply(determine_score, axis=1)\n",
    "result_mt['frameaxis_mfd2'] = frameaxis_mfd2.apply(determine_score, axis=1)\n",
    "result_mt['frameaxis_emfd'] = frameaxis_emfd.apply(determine_score, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### MoralStrength #####\n",
    "# note: moral_strength results are already processed, so here we just col-bind it to the result_mt dataframe\n",
    "moral_strength = pd.read_csv('/home/misinfo/turing_dso_misinfo/llms_mft_multilingual/paper_linked_final/experiment_machine_translation/result_raw/bm_moral_strength.csv')\n",
    "\n",
    "# recode the label to align with the other results\n",
    "moral_strength['moral_strength_sup_svm'].value_counts()\n",
    "\n",
    "mapping_moral_strength = {\n",
    "    \"care\": \"care\",\n",
    "    \"fairness\": \"fair\",\n",
    "    \"authority\": \"auth\",\n",
    "    \"loyalty\": \"loya\",\n",
    "    \"purity\": \"sanc\",\n",
    "    \"non-moral\": \"non_moral\"\n",
    "}\n",
    "\n",
    "moral_strength['moral_strength_sup_svm'] = moral_strength['moral_strength_sup_svm'].map(mapping_moral_strength)\n",
    "moral_strength['moral_strength_sup_svm2'] = moral_strength['moral_strength_sup_svm2'].map(mapping_moral_strength)\n",
    "moral_strength['moral_strength_unsup'] = moral_strength['moral_strength_unsup'].map(mapping_moral_strength)\n",
    "\n",
    "\n",
    "result_mt['moral_strength_unsup'] = moral_strength['moral_strength_unsup']\n",
    "result_mt['moral_strength_sup_svm'] = moral_strength['moral_strength_sup_svm']\n",
    "result_mt['moral_strength_sup_svm2'] = moral_strength['moral_strength_sup_svm2']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean 3. Language Models: Mformers and MoralBert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### clean - mformer #####\n",
    "mformer = pd.read_csv('/home/misinfo/turing_dso_misinfo/llms_mft_multilingual/paper_linked_final/experiment_machine_translation/result_raw/result_mformer.csv')\n",
    "\n",
    "new_column_names_mformer = {\n",
    "    'id': 'text',\n",
    "    'care_prob': 'care',\n",
    "    'fairness_prob': 'fair',\n",
    "    'loyalty_prob': 'loya',\n",
    "    'authority_prob': 'auth',\n",
    "    'sanctity_prob': 'sanc'\n",
    "}\n",
    "mformer = mformer.rename(columns=new_column_names_mformer)\n",
    "\n",
    "# decide the score label for each text \n",
    "columns_to_check = ['care', 'auth', 'fair', 'loya', 'sanc']\n",
    "\n",
    "# Function to determine the value for the new column\n",
    "def determine_score(row):\n",
    "    values = row[columns_to_check]\n",
    "    if values.max() == 0:\n",
    "        return \"non_moral\"\n",
    "    else:\n",
    "        max_value = values.max()\n",
    "        max_columns = [col for col in columns_to_check if row[col] == max_value]\n",
    "        return ', '.join(max_columns)\n",
    "    \n",
    "# save it to the result dataframe\n",
    "result_mt['mformer'] = mformer.apply(determine_score, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### clean - moralbert #####\n",
    "moralbert = pd.read_csv('/home/misinfo/turing_dso_misinfo/llms_mft_multilingual/paper_linked_final/experiment_machine_translation/result_raw/result_moralbert.csv')\n",
    "\n",
    "# aggregate the vice/virtue scores\n",
    "moralbert['care'] = moralbert['care'] + moralbert['harm']\n",
    "moralbert['fair'] = moralbert['fairness'] + moralbert['cheating']\n",
    "moralbert['loya'] = moralbert['loyalty'] + moralbert['betrayal']\n",
    "moralbert['auth'] = moralbert['authority'] + moralbert['subversion']\n",
    "moralbert['sanc'] = moralbert['purity'] + moralbert['degradation']\n",
    "\n",
    "# only keep necessary columns\n",
    "moralbert = moralbert[['sentence', 'care', 'auth', 'fair', 'loya', 'sanc']]\n",
    "# decide the score label for each text \n",
    "columns_to_check = ['care', 'auth', 'fair', 'loya', 'sanc']\n",
    "\n",
    "# save it to the result dataframe\n",
    "result_mt['moralbert'] = moralbert.apply(determine_score, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### save the result to the csv file ####\n",
    "result_mt.to_csv('/home/misinfo/turing_dso_misinfo/llms_mft_multilingual/paper_linked_final/experiment_machine_translation/result/result_mt_0912.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_mt = pd.read_csv('/home/misinfo/turing_dso_misinfo/llms_mft_multilingual/paper_linked_final/experiment_machine_translation/result/result_mt_0912.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Define a function to deal with multiple matches for one document ######\n",
    "\n",
    "\n",
    "def multi_match_split(df, bm_column, predict_column):\n",
    "    \"\"\"\n",
    "    this function will generate two predicted results:\n",
    "    fuzzy match means it will be a match if there is at least one value matched in the prediction (more true values)\n",
    "    exact match means it will only be a match if the whole value is matched in the prediction (fewer true values)\n",
    "    for non-moral values, it will remain as \"non_moral\" and counted in the coverage calculation, but not in the F1 score calculation\n",
    "    \"\"\"\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        predict_value = row[predict_column]\n",
    "        bm_value = row[bm_column]\n",
    "\n",
    "        # if No commas, meaning there is only one value predicted by the model, copy the value to both columns\n",
    "        if ',' not in predict_value:\n",
    "            df.loc[index, f\"{predict_column}_fuzzy_match\"] = predict_value\n",
    "            df.loc[index, f\"{predict_column}_exact_match\"] = predict_value\n",
    "        else:\n",
    "            \n",
    "            # Multiple values, split by comma\n",
    "            predict_value = predict_value.split(',')\n",
    "\n",
    "            # very unlikely for the model to code a text to non-moral and moral values at the same time, but just in case\n",
    "            if \"non_moral\" in predict_value:\n",
    "                df.loc[index, f\"{predict_column}_fuzzy_match\"] = \"non_moral\"\n",
    "                df.loc[index, f\"{predict_column}_exact_match\"] = \"non_moral\"\n",
    "            else:  \n",
    "                if bm_value in predict_value:\n",
    "                    # If source_label is in the mfd values - it is a match\n",
    "                    df.loc[index, f\"{predict_column}_fuzzy_match\"] = bm_value\n",
    "                    # Select one of the other values\n",
    "                    exact_match_value = next((val for val in predict_value if val != bm_value))\n",
    "                    df.loc[index, f\"{predict_column}_exact_match\"] = exact_match_value\n",
    "                else:\n",
    "                    # If no match is found, select the first value for both columns\n",
    "                    df.loc[index, f\"{predict_column}_exact_match\"] = predict_value[0]\n",
    "                    df.loc[index, f\"{predict_column}_fuzzy_match\"] = predict_value[0]\n",
    "    # trim the spaces before and after the string in the columns\n",
    "    df.loc[:,f\"{predict_column}_exact_match\"] = df[f\"{predict_column}_exact_match\"].str.strip()\n",
    "    df.loc[:,f\"{predict_column}_fuzzy_match\"] = df[f\"{predict_column}_fuzzy_match\"].str.strip()\n",
    "    # df[f\"{predict_column}\"] = df[f\"{predict_column}\"].str.strip()\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Define a function to calculate model performance ######\n",
    "\n",
    "def model_performance_coverage(df, predicted_label_column):\n",
    "    \"\"\"\n",
    "    This function will calculate the coverage of the model performance\n",
    "    coverage is the percentage of the non-moral text predicted by the model, meaning the limitation of the model performance\n",
    "    \"\"\"\n",
    "    # non_moral label is the label for the non-moral text\n",
    "    non_moral_label = \"non_moral\"\n",
    "    value_counts = df[predicted_label_column].value_counts()\n",
    "    non_moral_count = value_counts.get(non_moral_label, 0)\n",
    "    # show the percentage of the coverage\n",
    "    coverage = (len(df) - non_moral_count) / len(df)\n",
    "    # keep 4 digits after the decimal point\n",
    "    coverage = round(coverage, 2)\n",
    "\n",
    "    return coverage\n",
    "\n",
    "#### Function to round the values in the classification report dictionary #####\n",
    "def round_classification_report(report, digits=3):\n",
    "    for key, value in report.items():\n",
    "        if isinstance(value, dict):\n",
    "            for sub_key, sub_value in value.items():\n",
    "                report[key][sub_key] = round(sub_value, digits)\n",
    "        else:\n",
    "            report[key] = round(value, digits)\n",
    "    return report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_performance(df, source_label_column, predicted_label_column):\n",
    "    \n",
    "    \"\"\"\n",
    "    This function will return a dataframe, containing the classification report, model's name and coverage figure\n",
    "    \"\"\"\n",
    "    # calculate the coverage of the model\n",
    "    model_coverage = model_performance_coverage(df, predicted_label_column)\n",
    "    \n",
    "    # then, remove non-moral values from the classification report as it is calculated in the coverage calculation\n",
    "    df = df[df[predicted_label_column]!='non_moral']\n",
    "    true_labels = df[source_label_column]\n",
    "    predicted_labels = df[predicted_label_column]\n",
    "\n",
    "    # Generate classification report\n",
    "    class_report = classification_report(true_labels, predicted_labels, output_dict=True, zero_division=0)\n",
    "    rounded_class_report = round_classification_report(class_report, digits=2)\n",
    "    # Convert classification report to DataFrame\n",
    "    report_df = pd.DataFrame(rounded_class_report)\n",
    "    report_df = report_df.loc[['f1-score']]\n",
    "    report_df = report_df.rename(index={'f1-score': f'f1 {predicted_label_column}'})\n",
    "    \n",
    "    # add coverage figure to a new column, so far there should be only one row in the dataframe\n",
    "    report_df['model_coverage'] = model_coverage\n",
    "    return report_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "source_dataset  source_label\n",
       "BM_CS           care            389\n",
       "                auth            331\n",
       "                fair            259\n",
       "                loya            248\n",
       "                sanc            226\n",
       "BM_CV           care            619\n",
       "                loya            349\n",
       "                auth            271\n",
       "                fair            253\n",
       "                sanc             52\n",
       "BM_MFV          care             27\n",
       "                auth             25\n",
       "                loya             16\n",
       "                fair             12\n",
       "                sanc             10\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_mt.groupby('source_dataset')['source_label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "######## ADD the random baseline to each benchmark dataset ########\n",
    "    # Ground truth label distribution\n",
    "label_BM_MFV = pd.Series({'care': 27,'auth': 25, 'loya': 16, 'fair': 12,'sanc': 10})\n",
    "label_BM_CS =  pd.Series({'care': 389,'auth': 331, 'loya': 248, 'fair': 259,'sanc': 226})\n",
    "label_BM_CV = pd.Series({'care': 619,'auth': 271, 'loya': 349, 'fair': 253,'sanc': 52})\n",
    "\n",
    "def randome_baseline_performance(source_dataset):\n",
    "    label_counts_str = f\"label_{source_dataset}\"\n",
    "    \n",
    "    label_counts = eval(label_counts_str)\n",
    "    total_samples = label_counts.sum()\n",
    "\n",
    "    # Initialize an empty list to store random classification reports\n",
    "    classification_reports = []\n",
    "\n",
    "    classification_reports = []\n",
    "    for _ in range(1000):\n",
    "            random_predictions = np.random.choice(label_counts.index, size=total_samples, p=label_counts / total_samples)\n",
    "            true_labels = np.repeat(label_counts.index, label_counts.values)\n",
    "            \n",
    "            # Generate classification report for random predictions\n",
    "            random_classification_report = classification_report(true_labels, random_predictions, output_dict=True, zero_division=0)\n",
    "            # # classification_reports.append(random_classification_report)\n",
    "            # ramdom_rounded_class_report = round_classification_report(random_classification_report, digits=2)\n",
    "            random_report_df = pd.DataFrame(random_classification_report)\n",
    "            # extract all keys and its corresponding 'f1-score' values if it has otherwise, any value\n",
    "            extracted_values = {key: (value['f1-score'] if isinstance(value, dict) and 'f1-score' in value else value)\n",
    "                        for key, value in random_classification_report.items()}\n",
    "\n",
    "            classification_reports.append(extracted_values)\n",
    "\n",
    "        # Average the classification reports\n",
    "    classification_reports_df = pd.DataFrame(classification_reports)\n",
    "\n",
    "    # calculate the average of each column in the dataframe\n",
    "    avg_random_report = classification_reports_df.mean().to_frame().T\n",
    "    random_report_df = avg_random_report.rename(index={0: 'f1 random baseline'})\n",
    "    random_report_df[\"model_coverage\"] = 1.00\n",
    "\n",
    "    # Convert classification report to DataFrame\n",
    "    random_report_df = round_classification_report(random_report_df, digits=2)\n",
    "    \n",
    "    return random_report_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "mt_models = ['freq_mfd', 'freq_mfd2', 'freq_emfd', \n",
    "             'frameaxis_mfd', 'frameaxis_mfd2','frameaxis_emfd',\n",
    "             'moral_strength_unsup', 'moral_strength_sup_svm','moral_strength_sup_svm2',\n",
    "             'moralbert', 'mformer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### define a function to present the results in a table #######\n",
    "def present_tables_by_BM(df, source_dataset):\n",
    "    df = df[df['source_dataset'] == source_dataset]\n",
    "\n",
    "    table_display = pd.DataFrame()\n",
    "    # add the ramdom baseline to the table\n",
    "    random_baseline = randome_baseline_performance(source_dataset)\n",
    "    table_display = pd.concat([table_display, random_baseline], axis=0) # row bind\n",
    "    \n",
    "    for model_column in mt_models:\n",
    "        df =  multi_match_split(df, 'source_label', model_column)\n",
    "        df_row = model_performance(df, 'source_label', f'{model_column}_fuzzy_match')\n",
    "        table_display = pd.concat([table_display, df_row], axis=0) # row bind\n",
    "    \n",
    "    print(f\"Machine Translation MF Measurement Results Benchmarked with {source_dataset} Dataset\")\n",
    "    display(table_display)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>auth</th>\n",
       "      <th>care</th>\n",
       "      <th>fair</th>\n",
       "      <th>loya</th>\n",
       "      <th>sanc</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>macro avg</th>\n",
       "      <th>weighted avg</th>\n",
       "      <th>model_coverage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>f1 random baseline</th>\n",
       "      <td>0.28</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.23</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    auth  care  fair  loya  sanc  accuracy  macro avg  \\\n",
       "f1 random baseline  0.28   0.3  0.13  0.18  0.11      0.23        0.2   \n",
       "\n",
       "                    weighted avg  model_coverage  \n",
       "f1 random baseline          0.23             1.0  "
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "randome_baseline_performance(\"BM_MFV\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>auth</th>\n",
       "      <th>care</th>\n",
       "      <th>fair</th>\n",
       "      <th>loya</th>\n",
       "      <th>sanc</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>macro avg</th>\n",
       "      <th>weighted avg</th>\n",
       "      <th>model_coverage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>f1 random baseline</th>\n",
       "      <td>0.23</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.21</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    auth  care  fair  loya  sanc  accuracy  macro avg  \\\n",
       "f1 random baseline  0.23  0.27  0.18  0.17  0.16      0.21        0.2   \n",
       "\n",
       "                    weighted avg  model_coverage  \n",
       "f1 random baseline          0.21             1.0  "
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "randome_baseline_performance(\"BM_CS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>auth</th>\n",
       "      <th>care</th>\n",
       "      <th>fair</th>\n",
       "      <th>loya</th>\n",
       "      <th>sanc</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>macro avg</th>\n",
       "      <th>weighted avg</th>\n",
       "      <th>model_coverage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>f1 random baseline</th>\n",
       "      <td>0.18</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.27</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    auth  care  fair  loya  sanc  accuracy  macro avg  \\\n",
       "f1 random baseline  0.18   0.4  0.16  0.23  0.03      0.27        0.2   \n",
       "\n",
       "                    weighted avg  model_coverage  \n",
       "f1 random baseline          0.27             1.0  "
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "randome_baseline_performance(\"BM_CV\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table Analysis \n",
    "\n",
    "**NOTE** The baseline random model should be interpreted with caution. As labels in each benchmark dataset are very unevenly distributed. the current table is only for reference for now. For the publication purpose, the random model should be randomized maybe 10+ more times and take the average. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Machine Translation MF Measurement Results Benchmarked with BM_MFV Dataset\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>auth</th>\n",
       "      <th>care</th>\n",
       "      <th>fair</th>\n",
       "      <th>loya</th>\n",
       "      <th>sanc</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>macro avg</th>\n",
       "      <th>weighted avg</th>\n",
       "      <th>model_coverage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>f1 random baseline</th>\n",
       "      <td>0.27</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.22</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1 freq_mfd_fuzzy_match</th>\n",
       "      <td>0.77</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1 freq_mfd2_fuzzy_match</th>\n",
       "      <td>0.69</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1 freq_emfd_fuzzy_match</th>\n",
       "      <td>0.39</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.41</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1 frameaxis_mfd_fuzzy_match</th>\n",
       "      <td>0.39</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.21</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1 frameaxis_mfd2_fuzzy_match</th>\n",
       "      <td>0.19</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.21</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1 frameaxis_emfd_fuzzy_match</th>\n",
       "      <td>0.07</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.28</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1 moral_strength_unsup_fuzzy_match</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1 moral_strength_sup_svm_fuzzy_match</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1 moral_strength_sup_svm2_fuzzy_match</th>\n",
       "      <td>0.07</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.16</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1 moralbert_fuzzy_match</th>\n",
       "      <td>0.27</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.36</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1 mformer_fuzzy_match</th>\n",
       "      <td>0.65</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.57</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        auth  care  fair  loya  sanc  \\\n",
       "f1 random baseline                      0.27  0.30  0.13  0.17  0.11   \n",
       "f1 freq_mfd_fuzzy_match                 0.77  0.29  0.00  0.55  0.00   \n",
       "f1 freq_mfd2_fuzzy_match                0.69  0.50  0.75  0.59  0.33   \n",
       "f1 freq_emfd_fuzzy_match                0.39  0.58  0.31  0.22  0.44   \n",
       "f1 frameaxis_mfd_fuzzy_match            0.39  0.06  0.00  0.29  0.31   \n",
       "f1 frameaxis_mfd2_fuzzy_match           0.19  0.18  0.06  0.43  0.17   \n",
       "f1 frameaxis_emfd_fuzzy_match           0.07  0.47  0.19  0.40  0.17   \n",
       "f1 moral_strength_unsup_fuzzy_match     0.00  0.00  0.00  0.33  0.31   \n",
       "f1 moral_strength_sup_svm_fuzzy_match    NaN   NaN   NaN   NaN   NaN   \n",
       "f1 moral_strength_sup_svm2_fuzzy_match  0.07  0.32  0.21  0.09  0.00   \n",
       "f1 moralbert_fuzzy_match                0.27  0.55  0.50  0.10  0.30   \n",
       "f1 mformer_fuzzy_match                  0.65  0.65  0.72  0.31  0.40   \n",
       "\n",
       "                                        accuracy  macro avg  weighted avg  \\\n",
       "f1 random baseline                          0.23       0.20          0.22   \n",
       "f1 freq_mfd_fuzzy_match                     0.56       0.32          0.55   \n",
       "f1 freq_mfd2_fuzzy_match                    0.59       0.57          0.60   \n",
       "f1 freq_emfd_fuzzy_match                    0.42       0.39          0.41   \n",
       "f1 frameaxis_mfd_fuzzy_match                0.27       0.21          0.21   \n",
       "f1 frameaxis_mfd2_fuzzy_match               0.22       0.21          0.21   \n",
       "f1 frameaxis_emfd_fuzzy_match               0.31       0.26          0.28   \n",
       "f1 moral_strength_unsup_fuzzy_match         0.17       0.13          0.13   \n",
       "f1 moral_strength_sup_svm_fuzzy_match       0.00        NaN          0.00   \n",
       "f1 moral_strength_sup_svm2_fuzzy_match      0.19       0.14          0.16   \n",
       "f1 moralbert_fuzzy_match                    0.38       0.34          0.36   \n",
       "f1 mformer_fuzzy_match                      0.58       0.55          0.57   \n",
       "\n",
       "                                        model_coverage  \n",
       "f1 random baseline                                1.00  \n",
       "f1 freq_mfd_fuzzy_match                           0.28  \n",
       "f1 freq_mfd2_fuzzy_match                          0.46  \n",
       "f1 freq_emfd_fuzzy_match                          1.00  \n",
       "f1 frameaxis_mfd_fuzzy_match                      1.00  \n",
       "f1 frameaxis_mfd2_fuzzy_match                     1.00  \n",
       "f1 frameaxis_emfd_fuzzy_match                     1.00  \n",
       "f1 moral_strength_unsup_fuzzy_match               0.20  \n",
       "f1 moral_strength_sup_svm_fuzzy_match             0.00  \n",
       "f1 moral_strength_sup_svm2_fuzzy_match            1.00  \n",
       "f1 moralbert_fuzzy_match                          1.00  \n",
       "f1 mformer_fuzzy_match                            1.00  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import warnings\n",
    "import numpy as np\n",
    "\n",
    "# Ignore specific warnings\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=pd.errors.SettingWithCopyWarning)\n",
    "\n",
    "present_tables_by_BM(result_mt, 'BM_MFV')\n",
    "### the NA values means NO one single label is predicted by the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Machine Translation MF Measurement Results Benchmarked with BM_CS Dataset\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>auth</th>\n",
       "      <th>care</th>\n",
       "      <th>fair</th>\n",
       "      <th>loya</th>\n",
       "      <th>sanc</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>macro avg</th>\n",
       "      <th>weighted avg</th>\n",
       "      <th>model_coverage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>f1 random baseline</th>\n",
       "      <td>0.23</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.21</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1 freq_mfd_fuzzy_match</th>\n",
       "      <td>0.47</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1 freq_mfd2_fuzzy_match</th>\n",
       "      <td>0.55</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1 freq_emfd_fuzzy_match</th>\n",
       "      <td>0.48</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1 frameaxis_mfd_fuzzy_match</th>\n",
       "      <td>0.45</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.38</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1 frameaxis_mfd2_fuzzy_match</th>\n",
       "      <td>0.39</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.47</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1 frameaxis_emfd_fuzzy_match</th>\n",
       "      <td>0.33</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.30</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1 moral_strength_unsup_fuzzy_match</th>\n",
       "      <td>0.16</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1 moral_strength_sup_svm_fuzzy_match</th>\n",
       "      <td>0.68</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1 moral_strength_sup_svm2_fuzzy_match</th>\n",
       "      <td>0.21</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.25</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1 moralbert_fuzzy_match</th>\n",
       "      <td>0.41</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.55</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1 mformer_fuzzy_match</th>\n",
       "      <td>0.65</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.69</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        auth  care  fair  loya  sanc  \\\n",
       "f1 random baseline                      0.23  0.27  0.18  0.17  0.16   \n",
       "f1 freq_mfd_fuzzy_match                 0.47  0.62  0.67  0.18  0.84   \n",
       "f1 freq_mfd2_fuzzy_match                0.55  0.71  0.71  0.61  0.71   \n",
       "f1 freq_emfd_fuzzy_match                0.48  0.52  0.48  0.44  0.25   \n",
       "f1 frameaxis_mfd_fuzzy_match            0.45  0.38  0.31  0.26  0.47   \n",
       "f1 frameaxis_mfd2_fuzzy_match           0.39  0.57  0.57  0.39  0.43   \n",
       "f1 frameaxis_emfd_fuzzy_match           0.33  0.42  0.28  0.17  0.22   \n",
       "f1 moral_strength_unsup_fuzzy_match     0.16  0.11  0.16  0.18  0.21   \n",
       "f1 moral_strength_sup_svm_fuzzy_match   0.68  0.71  0.77  0.85  0.61   \n",
       "f1 moral_strength_sup_svm2_fuzzy_match  0.21  0.39  0.28  0.15  0.14   \n",
       "f1 moralbert_fuzzy_match                0.41  0.63  0.58  0.64  0.49   \n",
       "f1 mformer_fuzzy_match                  0.65  0.71  0.70  0.71  0.71   \n",
       "\n",
       "                                        accuracy  macro avg  weighted avg  \\\n",
       "f1 random baseline                          0.21       0.20          0.21   \n",
       "f1 freq_mfd_fuzzy_match                     0.54       0.55          0.54   \n",
       "f1 freq_mfd2_fuzzy_match                    0.66       0.66          0.66   \n",
       "f1 freq_emfd_fuzzy_match                    0.47       0.43          0.45   \n",
       "f1 frameaxis_mfd_fuzzy_match                0.38       0.37          0.38   \n",
       "f1 frameaxis_mfd2_fuzzy_match               0.47       0.47          0.47   \n",
       "f1 frameaxis_emfd_fuzzy_match               0.30       0.28          0.30   \n",
       "f1 moral_strength_unsup_fuzzy_match         0.16       0.16          0.16   \n",
       "f1 moral_strength_sup_svm_fuzzy_match       0.74       0.72          0.74   \n",
       "f1 moral_strength_sup_svm2_fuzzy_match      0.26       0.23          0.25   \n",
       "f1 moralbert_fuzzy_match                    0.57       0.55          0.55   \n",
       "f1 mformer_fuzzy_match                      0.69       0.70          0.69   \n",
       "\n",
       "                                        model_coverage  \n",
       "f1 random baseline                                1.00  \n",
       "f1 freq_mfd_fuzzy_match                           0.60  \n",
       "f1 freq_mfd2_fuzzy_match                          0.75  \n",
       "f1 freq_emfd_fuzzy_match                          0.98  \n",
       "f1 frameaxis_mfd_fuzzy_match                      1.00  \n",
       "f1 frameaxis_mfd2_fuzzy_match                     1.00  \n",
       "f1 frameaxis_emfd_fuzzy_match                     1.00  \n",
       "f1 moral_strength_unsup_fuzzy_match               0.17  \n",
       "f1 moral_strength_sup_svm_fuzzy_match             0.23  \n",
       "f1 moral_strength_sup_svm2_fuzzy_match            1.00  \n",
       "f1 moralbert_fuzzy_match                          1.00  \n",
       "f1 mformer_fuzzy_match                            1.00  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "present_tables_by_BM(result_mt, 'BM_CS')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Machine Translation MF Measurement Results Benchmarked with BM_CV Dataset\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>auth</th>\n",
       "      <th>care</th>\n",
       "      <th>fair</th>\n",
       "      <th>loya</th>\n",
       "      <th>sanc</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>macro avg</th>\n",
       "      <th>weighted avg</th>\n",
       "      <th>model_coverage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>f1 random baseline</th>\n",
       "      <td>0.17</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.27</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1 freq_mfd_fuzzy_match</th>\n",
       "      <td>0.33</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1 freq_mfd2_fuzzy_match</th>\n",
       "      <td>0.23</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1 freq_emfd_fuzzy_match</th>\n",
       "      <td>0.11</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.36</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1 frameaxis_mfd_fuzzy_match</th>\n",
       "      <td>0.29</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.29</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1 frameaxis_mfd2_fuzzy_match</th>\n",
       "      <td>0.07</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.34</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1 frameaxis_emfd_fuzzy_match</th>\n",
       "      <td>0.04</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.34</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1 moral_strength_unsup_fuzzy_match</th>\n",
       "      <td>0.17</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1 moral_strength_sup_svm_fuzzy_match</th>\n",
       "      <td>0.28</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1 moral_strength_sup_svm2_fuzzy_match</th>\n",
       "      <td>0.12</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.23</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1 moralbert_fuzzy_match</th>\n",
       "      <td>0.15</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.41</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1 mformer_fuzzy_match</th>\n",
       "      <td>0.23</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.47</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        auth  care  fair  loya  sanc  \\\n",
       "f1 random baseline                      0.17  0.40  0.16  0.23  0.03   \n",
       "f1 freq_mfd_fuzzy_match                 0.33  0.40  0.43  0.30  0.00   \n",
       "f1 freq_mfd2_fuzzy_match                0.23  0.62  0.43  0.27  0.06   \n",
       "f1 freq_emfd_fuzzy_match                0.11  0.59  0.36  0.20  0.02   \n",
       "f1 frameaxis_mfd_fuzzy_match            0.29  0.30  0.27  0.31  0.06   \n",
       "f1 frameaxis_mfd2_fuzzy_match           0.07  0.49  0.35  0.30  0.11   \n",
       "f1 frameaxis_emfd_fuzzy_match           0.04  0.56  0.31  0.24  0.08   \n",
       "f1 moral_strength_unsup_fuzzy_match     0.17  0.21  0.23  0.23  0.09   \n",
       "f1 moral_strength_sup_svm_fuzzy_match   0.28  0.59  0.49  0.27  0.18   \n",
       "f1 moral_strength_sup_svm2_fuzzy_match  0.12  0.35  0.25  0.12  0.03   \n",
       "f1 moralbert_fuzzy_match                0.15  0.62  0.47  0.24  0.10   \n",
       "f1 mformer_fuzzy_match                  0.23  0.72  0.52  0.21  0.14   \n",
       "\n",
       "                                        accuracy  macro avg  weighted avg  \\\n",
       "f1 random baseline                          0.27       0.20          0.27   \n",
       "f1 freq_mfd_fuzzy_match                     0.34       0.29          0.35   \n",
       "f1 freq_mfd2_fuzzy_match                    0.43       0.32          0.43   \n",
       "f1 freq_emfd_fuzzy_match                    0.40       0.26          0.36   \n",
       "f1 frameaxis_mfd_fuzzy_match                0.28       0.25          0.29   \n",
       "f1 frameaxis_mfd2_fuzzy_match               0.34       0.27          0.34   \n",
       "f1 frameaxis_emfd_fuzzy_match               0.38       0.25          0.34   \n",
       "f1 moral_strength_unsup_fuzzy_match         0.20       0.19          0.21   \n",
       "f1 moral_strength_sup_svm_fuzzy_match       0.47       0.36          0.44   \n",
       "f1 moral_strength_sup_svm2_fuzzy_match      0.23       0.17          0.23   \n",
       "f1 moralbert_fuzzy_match                    0.45       0.32          0.41   \n",
       "f1 mformer_fuzzy_match                      0.50       0.36          0.47   \n",
       "\n",
       "                                        model_coverage  \n",
       "f1 random baseline                                1.00  \n",
       "f1 freq_mfd_fuzzy_match                           0.47  \n",
       "f1 freq_mfd2_fuzzy_match                          0.72  \n",
       "f1 freq_emfd_fuzzy_match                          1.00  \n",
       "f1 frameaxis_mfd_fuzzy_match                      1.00  \n",
       "f1 frameaxis_mfd2_fuzzy_match                     1.00  \n",
       "f1 frameaxis_emfd_fuzzy_match                     1.00  \n",
       "f1 moral_strength_unsup_fuzzy_match               0.36  \n",
       "f1 moral_strength_sup_svm_fuzzy_match             0.17  \n",
       "f1 moral_strength_sup_svm2_fuzzy_match            1.00  \n",
       "f1 moralbert_fuzzy_match                          1.00  \n",
       "f1 mformer_fuzzy_match                            1.00  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "present_tables_by_BM(result_mt, 'BM_CV')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
